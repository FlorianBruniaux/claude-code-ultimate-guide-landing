# Claude Code Quiz - Debug Export
# Total questions: 256
# Format: id, difficulty, category, question, options, correct, explanation

questions:
  - id: 01-001
    difficulty: junior
    category: "Unknown"
    question: |
      What is the recommended universal method to install Claude Code?
    options:
      - a) brew install claude-code
      - b) npm install -g @anthropic-ai/claude-code
      - c) pip install claude-code
      - d) curl -fsSL https://claude.ai/install.sh | sh [CORRECT]
    correct: d
    explanation: |
      The recommended installation method is `curl -fsSL https://claude.ai/install.sh | sh` which works across all platforms. npm is deprecated - use `claude install` to migrate if you installed via npm. While Homebrew is available for macOS, the shell script is the universal method recommended in official docs.

  - id: 01-002
    difficulty: junior
    category: "Unknown"
    question: |
      After Claude proposes a code change, what options do you have when reviewing the diff?
    options:
      - a) Only accept (y) or reject (n)
      - b) Accept (y), reject (n), or edit (e) [CORRECT]
      - c) Accept (y), skip (s), or delay (d)
      - d) Commit (c), reject (r), or review (v)
    correct: b
    explanation: |
      When Claude proposes a change, you have three options: press 'y' to accept the change, 'n' to reject and ask for alternatives, or 'e' to edit the change manually. This gives you full control over what gets applied to your codebase. Always review diffs before accepting - this is your safety net. 

  - id: 01-003
    difficulty: junior
    category: "Unknown"
    question: |
      Which command shows all available Claude Code commands?
    options:
      - a) /commands
      - b) /list
      - c) /help [CORRECT]
      - d) /menu
    correct: c
    explanation: |
      The `/help` command displays all available Claude Code commands. This is the go-to command when you're lost or want to discover available functionality. It's one of the 7 essential commands that cover 90% of daily usage. 

  - id: 01-004
    difficulty: junior
    category: "Unknown"
    question: |
      What does the `!` prefix do in Claude Code?
    options:
      - a) Marks a command as urgent
      - b) Runs a shell command directly without asking Claude [CORRECT]
      - c) Escapes special characters
      - d) Deletes the previous command
    correct: b
    explanation: |
      The `!` prefix executes shell commands immediately without asking Claude to do it. For example, `!git status` runs the command directly. Use this for quick status checks, view commands, and already-known commands. It's faster than asking Claude when you know exactly what command you need. 

  - id: 01-005
    difficulty: junior
    category: "Unknown"
    question: |
      What does the `@` symbol do when used in a prompt?
    options:
      - a) Mentions another user
      - b) References a specific file for targeted operations [CORRECT]
      - c) Tags a message as important
      - d) Activates an agent
    correct: b
    explanation: |
      The `@` symbol references specific files in your prompts for targeted operations. For example, `Review @src/auth/login.tsx for security issues` signals Claude to read that file. This provides precision (target exact files), speed (skip file discovery), and clarity (makes your intent explicit). Claude reads the file on-demand via tools. 

  - id: 01-006
    difficulty: senior
    category: "Unknown"
    question: |
      What is the recommended approach when migrating from GitHub Copilot to Claude Code?
    options:
      - a) Completely stop using Copilot immediately
      - b) Use a hybrid approach: Copilot for autocomplete, Claude Code for complex tasks [CORRECT]
      - c) Only use Claude Code for simple tasks
      - d) Export all Copilot settings to Claude Code
    correct: b
    explanation: |
      The guide recommends a hybrid approach: use Copilot for quick autocomplete and boilerplate while using Claude Code for feature implementation, debugging, code reviews, and understanding unfamiliar codebases. This leverages the strengths of both tools - Copilot excels at inline suggestions while Claude Code handles multi-file operations and complex reasoning. 

  - id: 01-007
    difficulty: junior
    category: "Unknown"
    question: |
      In Plan Mode, what is Claude allowed to do?
    options:
      - a) Edit files, run commands, and make commits
      - b) Only read and analyze - no modifications allowed [CORRECT]
      - c) Run commands but not edit files
      - d) Create new files but not edit existing ones
    correct: b
    explanation: |
      Plan Mode is Claude Code's "look but don't touch" mode. It allows reading files, searching the codebase, analyzing architecture, and proposing approaches. It prevents editing files, running state-modifying commands, creating new files, and making commits. Perfect for safe exploration before making changes. 

  - id: 01-008
    difficulty: senior
    category: "Unknown"
    question: |
      What flag allows you to continue your most recent Claude Code conversation?
    options:
      - a) --last
      - b) --continue or -c [CORRECT]
      - c) --resume-last
      - d) --restore
    correct: b
    explanation: |
      Use `claude --continue` or `claude -c` to automatically resume your most recent conversation. This maintains full context and conversation history across terminal sessions. For resuming a specific session by ID, use `claude --resume <id>` or `claude -r <id>`. This is particularly useful for multi-day features or when interrupted. 

  - id: 01-009
    difficulty: junior
    category: "Unknown"
    question: |
      What should you ALWAYS do before accepting a code change from Claude?
    options:
      - a) Run the test suite
      - b) Create a backup of the file
      - c) Read the diff carefully [CORRECT]
      - d) Ask for Claude's confidence level
    correct: c
    explanation: |
      Always read the diff before accepting changes - this is your safety net. The guide emphasizes this as critical: "Always review diffs before accepting changes." While running tests is good practice, reviewing the diff is the immediate required step before any acceptance. You need to understand what changes are being proposed. 

  - id: 01-010
    difficulty: junior
    category: "Unknown"
    question: |
      Which keyboard shortcut cancels the current operation in Claude Code?
    options:
      - a) Ctrl+Z
      - b) Ctrl+C [CORRECT]
      - c) Esc
      - d) Ctrl+Q
    correct: b
    explanation: |
      Ctrl+C cancels the current operation in Claude Code. This is useful for stopping long-running analysis or when Claude is taking an approach you don't want. Esc dismisses the current suggestion, while Ctrl+R retries the last operation. Knowing these shortcuts helps maintain control during your workflow. 

  - id: 01-011
    difficulty: senior
    category: "Unknown"
    question: |
      When should you use auto-accept mode in Claude Code?
    options:
      - a) For all operations to save time
      - b) Only for well-defined, reversible operations you trust [CORRECT]
      - c) When working on production code
      - d) For complex refactoring tasks
    correct: b
    explanation: |
      Auto-accept mode should only be used for well-defined, reversible operations. The guide warns: "Only use auto-accept for well-defined, reversible operations." It's dangerous for complex or risky changes. Default mode (asking permission) is safest, especially for learning and production work. 

  - id: 01-012
    difficulty: power
    category: "Unknown"
    question: |
      What is the recommended rule of thumb for choosing between Claude Code and autocomplete tools based on code size?
    options:
      - a) <10 lines: autocomplete, >10 lines: Claude Code
      - b) <5 lines: autocomplete, 5-50 lines single file: either, >50 lines or multi-file: Claude Code [CORRECT]
      - c) Always use Claude Code regardless of size
      - d) <20 lines: autocomplete, >20 lines: Claude Code
    correct: b
    explanation: |
      The guide provides clear guidance: less than 5 lines of code - use Copilot/autocomplete; 5-50 lines in a single file - either tool works; more than 50 lines or multi-file changes - use Claude Code. This helps you choose the right tool for the task's complexity level. 

  - id: 01-013
    difficulty: junior
    category: "Unknown"
    question: |
      What image formats does Claude Code support for visual analysis?
    options:
      - a) Only PNG
      - b) PNG, JPG, JPEG, WebP, GIF (static) [CORRECT]
      - c) All image formats including RAW
      - d) SVG and PNG only
    correct: b
    explanation: |
      Claude Code supports PNG, JPG, JPEG, WebP, and static GIF formats. You can paste images directly in the terminal (Cmd+V/Ctrl+V), drag and drop, or reference by path. This is useful for implementing UI from mockups, debugging visual issues, analyzing diagrams, and accessibility audits. Note that images consume significant context tokens (1000-2000 words equivalent). 

  - id: 01-014
    difficulty: senior
    category: "Unknown"
    question: |
      When resuming a Claude Code session, what context is preserved?
    options:
      - a) Only the conversation history
      - b) Full conversation history, files read/edited, CLAUDE.md settings [CORRECT]
      - c) Just the last 10 messages
      - d) Only files that were modified
    correct: b
    explanation: |
      When you resume a session, Claude retains: full conversation history, files previously read/edited, CLAUDE.md and project settings, and uncommitted code changes awareness. MCP servers restart on each session - their state is NOT preserved. Session-scoped permissions also don't carry over.

  - id: 01-015
    difficulty: junior
    category: "Unknown"
    question: |
      What command should you use to verify Claude Code installation?
    options:
      - a) claude check
      - b) claude --verify
      - c) claude --version [CORRECT]
      - d) claude test
    correct: c
    explanation: |
      Use `claude --version` to verify your installation and display the current version. This is also useful before reporting bugs. After installation, you can also use `claude doctor` to verify auto-updater health and `claude update` to check for available updates. 

  - id: 01-016
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the optimal image resolution range for Claude Code visual analysis?
    options:
      - a) 50-200px (smallest possible)
      - b) 200-1568px (sweet spot for quality/token balance) [CORRECT]
      - c) 2000-4000px (maximum detail)
      - d) 8000px+ (highest resolution)
    correct: b
    explanation: |
      The optimal range is 200-1568px. Below 200px lacks detail, 200-1000px is the sweet spot for wireframes, 1000-1568px provides optimal quality/token balance, images above 1568px are auto-downscaled (wasting upload time), and images over 8000px are rejected by the API. 

  - id: 01-017
    difficulty: senior
    category: "Unknown"
    question: |
      How are image tokens calculated in Claude Code?
    options:
      - a) file_size_bytes / 1000
      - b) (width × height) / 750 [CORRECT]
      - c) pixels / 1000
      - d) Fixed 500 tokens per image
    correct: b
    explanation: |
      Token calculation formula: (width × height) / 750 ≈ tokens consumed. Examples: 200×200 = ~54 tokens, 500×500 = ~334 tokens, 1000×1000 = ~1334 tokens. This helps estimate context impact before pasting images. Use /status after pasting to monitor actual context usage. 

  - id: 01-018
    difficulty: intermediate
    category: "Unknown"
    question: |
      Which image format is recommended for wireframes and diagrams in Claude Code?
    options:
      - a) JPEG (best compression)
      - b) PNG (sharp lines and text) [CORRECT]
      - c) GIF (universal support)
      - d) BMP (lossless)
    correct: b
    explanation: |
      PNG is recommended for wireframes, diagrams, and text because it preserves sharp lines. WebP is good for general screenshots with compression. JPEG is for photos only—compression artifacts harm line detection. GIF should be avoided (static only, poor quality). Always crop to relevant area and resize to 1000-1200px if larger. 

  - id: 02-001
    difficulty: junior
    category: "Unknown"
    question: |
      At what context percentage should you use /compact?
    options:
      - a) 0-50%
      - b) 50-70%
      - c) 70-90% [CORRECT]
      - d) Only at 100%
    correct: c
    explanation: |
      Use /compact when context reaches 70-90% (the red zone). The context zones are: Green (0-50%) - work freely; Yellow (50-75%) - start being selective; Red (75-90%) - use /compact; Critical (90%+) - must /clear or risk errors. Using /compact at 70% reduces usage by approximately 50% while preserving key context. 

  - id: 02-002
    difficulty: junior
    category: "Unknown"
    question: |
      What is Claude's context window size?
    options:
      - a) 50,000 tokens
      - b) 100,000 tokens
      - c) 200,000 tokens [CORRECT]
      - d) 500,000 tokens
    correct: c
    explanation: |
      Claude has a 200,000 token context window. Think of it like RAM - when it fills up, things slow down or fail. This context includes all messages, files read, command outputs, and tool results. Effective context management is described as "the most important concept in Claude Code." 

  - id: 02-003
    difficulty: senior
    category: "Unknown"
    question: |
      What does the statusline 'Ctx(u): 45%' indicate?
    options:
      - a) 45% of your budget is remaining
      - b) You've used 45% of your context [CORRECT]
      - c) 45% of files are loaded
      - d) Claude is 45% confident
    correct: b
    explanation: |
      The statusline metric 'Ctx(u): 45%' shows you've used 45% of your context window. The full statusline format is: `Claude Code | Ctx(u): 45% | Cost: $0.23 | Session: 1h 23m`. Monitoring this helps you know when to use /compact or /clear before context-related issues occur. 

  - id: 02-004
    difficulty: junior
    category: "Unknown"
    question: |
      What is the difference between /compact and /clear?
    options:
      - a) /compact is faster, /clear is more thorough
      - b) /compact summarizes context (preserves key info), /clear starts completely fresh (loses all context) [CORRECT]
      - c) /compact clears files, /clear clears conversations
      - d) They do the same thing
    correct: b
    explanation: |
      /compact summarizes the conversation, preserving key context while reducing usage by approximately 50%. Use it when running low on context. /clear starts completely fresh, losing all context - use it when changing topics or context is severely bloated. Choose based on whether you need to maintain continuity. 

  - id: 02-005
    difficulty: senior
    category: "Unknown"
    question: |
      Which action consumes the MOST context tokens?
    options:
      - a) Reading a small file (~500 tokens)
      - b) Running a simple command (~1K tokens)
      - c) Reading a large file or multi-file search (~5K+ tokens) [CORRECT]
      - d) Sending a short message
    correct: c
    explanation: |
      Reading large files (5K+ tokens) and multi-file searches (3K+ tokens) consume the most context. A small file costs ~500 tokens, running commands ~1K tokens. Long conversations accumulate over time. To optimize, be specific in queries, use symbol references like "read the calculateTotal function" instead of entire files. 

  - id: 02-006
    difficulty: power
    category: "Unknown"
    question: |
      What is the 'Last 20% Rule' in context management?
    options:
      - a) Always keep the last 20% of your conversation
      - b) Reserve ~20% of context for multi-file operations, corrections, and summary generation at session end [CORRECT]
      - c) Delete 20% of context regularly
      - d) Use only 20% of available context
    correct: b
    explanation: |
      The Last 20% Rule recommends reserving approximately 20% of your context for: multi-file operations at end of session, last-minute corrections, and generating summary/checkpoint documents. This buffer ensures you can complete your work properly even as context fills up. 

  - id: 02-007
    difficulty: senior
    category: "Unknown"
    question: |
      What is 'context poisoning' (also called context bleeding)?
    options:
      - a) When context usage reaches 100%
      - b) When information from one task contaminates another [CORRECT]
      - c) When Claude forgets your instructions
      - d) When files get corrupted
    correct: b
    explanation: |
      Context poisoning occurs when information from one task contaminates another. Examples include: style bleeding (blue button style applies to unrelated forms), instruction contamination (conflicting rules cause confusion), and temporal confusion (Claude uses outdated file names). Use explicit task boundaries and clarify priorities to prevent it. 

  - id: 02-008
    difficulty: junior
    category: "Unknown"
    question: |
      What is the 7-step interaction loop in Claude Code?
    options:
      - a) Plan, Code, Test, Deploy, Monitor, Fix, Repeat
      - b) Describe, Analyze, Propose, Review, Decide, Verify, Commit [CORRECT]
      - c) Read, Write, Edit, Run, Debug, Test, Push
      - d) Ask, Wait, Accept, Run, Check, Fix, Done
    correct: b
    explanation: |
      The interaction loop is: 1) DESCRIBE - explain what you need, 2) ANALYZE - Claude explores the codebase, 3) PROPOSE - Claude suggests changes (diff), 4) REVIEW - you read and evaluate, 5) DECIDE - Accept/Reject/Modify, 6) VERIFY - run tests, check behavior, 7) COMMIT - save changes (optional). The key insight: you remain in control throughout. 

  - id: 02-009
    difficulty: junior
    category: "Unknown"
    question: |
      How do you exit Plan Mode to start making changes?
    options:
      - a) /edit
      - b) /execute [CORRECT]
      - c) /start
      - d) /exit-plan
    correct: b
    explanation: |
      Use /execute to exit Plan Mode and begin implementing changes. While in Plan Mode, Claude can only read and analyze - no modifications are allowed. You can also respond to Claude's prompt "Ready to implement this plan?" to transition. This staged approach ensures you understand the plan before execution. 

  - id: 02-010
    difficulty: power
    category: "Unknown"
    question: |
      What is OpusPlan mode?
    options:
      - a) A premium subscription tier
      - b) Using Opus for planning (superior reasoning) and Sonnet for implementation (cost-efficient) [CORRECT]
      - c) A debugging mode
      - d) A way to plan without Claude
    correct: b
    explanation: |
      OpusPlan uses Opus for planning (with superior reasoning capabilities) and automatically switches to Sonnet for implementation (more cost-efficient). Enable with `/model opusplan`. This provides Opus-quality planning while preserving tokens through Sonnet-speed execution. Particularly valuable for Pro subscribers with limited Opus tokens. 

  - id: 02-011
    difficulty: senior
    category: "Unknown"
    question: |
      What are symptoms of context depletion?
    options:
      - a) Claude types slower
      - b) Shorter responses, forgetting CLAUDE.md instructions, inconsistencies with earlier conversation [CORRECT]
      - c) Error messages appear
      - d) Cost increases dramatically
    correct: b
    explanation: |
      Context depletion symptoms include: shorter responses than usual (warning), forgetting CLAUDE.md instructions (serious), inconsistencies with earlier conversation (critical), errors on code already discussed (critical), and "I can't access that file" for files already read (critical). When critical symptoms appear, start a new session immediately. 

  - id: 02-012
    difficulty: junior
    category: "Unknown"
    question: |
      What can /rewind do?
    options:
      - a) Undo git commits
      - b) Revert Claude's file changes within the current session [CORRECT]
      - c) Go back in conversation history
      - d) Restore deleted files from disk
    correct: b
    explanation: |
      /rewind reverts file changes made by Claude. It works across multiple files but only on Claude's changes (not manual edits), only within the current session, and does NOT automatically revert git commits. For risky operations, create a git checkpoint first: "Let's commit what we have before trying this experimental approach." 

  - id: 02-013
    difficulty: senior
    category: "Unknown"
    question: |
      What does Claude NOT have access to regarding your project?
    options:
      - a) File structure and code content
      - b) Git state and branches
      - c) Runtime state, external services, and hidden files [CORRECT]
      - d) Project rules in CLAUDE.md
    correct: c
    explanation: |
      Claude knows: file structure, code content, git state, and project rules (CLAUDE.md). Claude does NOT know: runtime state (can't see running processes), external services (can't access databases directly), your intent (needs clear instructions), and hidden files (respects .gitignore by default). Understanding this mental model helps you communicate effectively. 

  - id: 02-014
    difficulty: power
    category: "Unknown"
    question: |
      What is the purpose of the 'Sanity Check Technique'?
    options:
      - a) To verify code compiles correctly
      - b) To verify Claude has loaded your CLAUDE.md configuration correctly [CORRECT]
      - c) To check for memory leaks
      - d) To validate test coverage
    correct: b
    explanation: |
      The Sanity Check Technique verifies Claude loaded your configuration. Add identifiable info to CLAUDE.md (like your name, project name, tech stack), then ask Claude "What is my name? What project am I working on?" Correct answers confirm configuration is loaded. For advanced checking, add multiple checkpoints throughout long CLAUDE.md files. 

  - id: 02-015
    difficulty: senior
    category: "Unknown"
    question: |
      When should you use XML-structured prompts?
    options:
      - a) For all prompts regardless of complexity
      - b) For multi-step features, bug investigations with context, and code reviews with specific criteria [CORRECT]
      - c) Only for simple one-liner requests
      - d) Only when working with APIs
    correct: b
    explanation: |
      Use XML-structured prompts when requests have 3+ distinct aspects (instruction + context + constraints), when ambiguity causes misunderstanding, when creating reusable templates, or for complex hierarchy. Don't use them for simple one-liner requests or quick typo fixes - the overhead outweighs the benefit. Tags like <instruction>, <context>, <constraints> help Claude understand different aspects. 

  - id: 02-016
    difficulty: power
    category: "Unknown"
    question: |
      What is a 'Session Handoff Pattern' used for?
    options:
      - a) Transferring sessions between team members
      - b) Documenting state, decisions, and next steps to maintain continuity between sessions [CORRECT]
      - c) Backing up your code
      - d) Exporting conversation history
    correct: b
    explanation: |
      The Session Handoff Pattern creates a document to bridge gaps between sessions. It includes: what was accomplished, current state, decisions made, next steps, and context for the next session. Create handoffs at end of work day, before context limit, when switching focus areas, or during interruptions. Store in `claudedocs/handoffs/handoff-YYYY-MM-DD.md`. 

  - id: 02-017
    difficulty: senior
    category: "Unknown"
    question: |
      What is the typical cost of a 1-hour Claude Code session?
    options:
      - a) $0.01 - $0.05
      - b) $0.10 - $0.50 [CORRECT]
      - c) $1.00 - $5.00
      - d) $10.00 - $20.00
    correct: b
    explanation: |
      A typical 1-hour session costs $0.10 - $0.50 depending on usage patterns. The guide provides cost budgets: quick task (5-10 min) $0.05-$0.10, feature work (1-2 hours) $0.20-$0.50, deep refactor (half day) $1.00-$2.00. Spending $0.50 to save 30 minutes provides 60x ROI if your time is worth $30/hour - don't over-optimize! 

  - id: 02-018
    difficulty: power
    category: "Unknown"
    question: |
      What is Auto Plan Mode and how do you enable it?
    options:
      - a) Automatic planning that's always on by default
      - b) A configuration that forces Claude to present a plan and wait for approval before any tool execution [CORRECT]
      - c) A mode that automatically generates project plans
      - d) A premium feature for enterprise users
    correct: b
    explanation: |
      Auto Plan Mode makes Claude present a plan and wait for explicit user approval before executing ANY tool. Configure via `~/.claude/auto-plan-mode.txt` and launch with `claude --append-system-prompt "$(cat ~/.claude/auto-plan-mode.txt)"`. Results in 76% fewer tokens with better results because plans are validated before execution. 

  - id: 03-001
    difficulty: junior
    category: "Unknown"
    question: |
      What is the correct precedence order for CLAUDE.md files (highest to lowest priority)?
    options:
      - a) Global > Project > Local
      - b) Local (.claude/CLAUDE.md) > Project (/project/CLAUDE.md) > Global (~/.claude/CLAUDE.md) [CORRECT]
      - c) Project > Local > Global
      - d) All CLAUDE.md files have equal priority
    correct: b
    explanation: |
      The precedence is: Local (.claude/CLAUDE.md) > Project (/project/CLAUDE.md) > Global (~/.claude/CLAUDE.md). More specific beats more general. Local overrides are personal (gitignored), project settings are shared (committed), and global settings apply to all projects. This hierarchy allows personal preferences to override team conventions when needed. 

  - id: 03-002
    difficulty: junior
    category: "Unknown"
    question: |
      Where should team conventions be stored to be shared via version control?
    options:
      - a) ~/.claude/CLAUDE.md
      - b) /project/.claude/CLAUDE.md
      - c) /project/CLAUDE.md [CORRECT]
      - d) /project/.claude/settings.local.json
    correct: c
    explanation: |
      Team conventions should be stored in `/project/CLAUDE.md` (the project root). This file is committed to git and shared with the team. Local overrides go in `/project/.claude/CLAUDE.md` (gitignored), and personal global preferences go in `~/.claude/CLAUDE.md`. This separation ensures team standards are enforced while allowing personal customization. 

  - id: 03-003
    difficulty: senior
    category: "Unknown"
    question: |
      What is the purpose of settings.local.json?
    options:
      - a) Store team hook configurations
      - b) Define project-wide settings
      - c) Personal permission overrides (gitignored) [CORRECT]
      - d) Configure MCP servers for the team
    correct: c
    explanation: |
      settings.local.json stores personal permission overrides and is gitignored. It allows you to customize which tools are auto-allowed, denied, or require asking for your personal workflow without affecting team settings. For example, you might allow all git commands while the team requires confirmation for certain operations. 

  - id: 03-004
    difficulty: junior
    category: "Unknown"
    question: |
      Which folder contains custom agents, commands, hooks, and skills?
    options:
      - a) ~/.claude/
      - b) /project/CLAUDE.md
      - c) /project/.claude/ [CORRECT]
      - d) /project/config/
    correct: c
    explanation: |
      The `.claude/` folder in your project contains: agents/ (custom agent definitions), commands/ (custom slash commands), hooks/ (event-driven scripts), skills/ (knowledge modules), rules/ (auto-loaded conventions), and settings files. This is your project's Claude Code configuration directory for all extensions. 

  - id: 03-005
    difficulty: senior
    category: "Unknown"
    question: |
      What is the permission behavior for tools listed in the 'deny' category?
    options:
      - a) Ask for confirmation each time
      - b) Auto-approve without asking
      - c) Block completely [CORRECT]
      - d) Log but allow
    correct: c
    explanation: |
      Tools in the 'deny' category are blocked completely - Claude cannot use them at all. The three permission behaviors are: 'allow' (auto-approve without asking), 'deny' (block completely), and 'ask' (prompt for confirmation). For example, denying "Bash(rm -rf *)" prevents accidental destructive operations. 

  - id: 03-006
    difficulty: power
    category: "Unknown"
    question: |
      What is the 'Single Source of Truth Pattern' for multi-tool AI setups?
    options:
      - a) Use only one AI tool per project
      - b) Store conventions in docs/conventions/ and reference them from all AI tool configs [CORRECT]
      - c) Copy the same rules to each AI tool's config
      - d) Let each AI tool define its own rules
    correct: b
    explanation: |
      The Single Source of Truth Pattern stores conventions in `/docs/conventions/` (coding-standards.md, architecture.md, testing.md, etc.) and references them from CLAUDE.md, CodeRabbit, and other tools. This prevents conflicts where one tool approves code that another flags. All tools enforce the same standards from one source. 

  - id: 03-007
    difficulty: junior
    category: "Unknown"
    question: |
      What should be included in a project-level CLAUDE.md file?
    options:
      - a) Only personal preferences
      - b) Tech stack, code conventions, architecture patterns, and common commands [CORRECT]
      - c) API keys and secrets
      - d) Git history
    correct: b
    explanation: |
      Project CLAUDE.md should include: tech stack (frameworks, versions), code conventions (naming, patterns), architecture (folder structure, layers), and common commands (dev, test, lint). This gives Claude project context. Never include API keys or secrets. Keep it concise with examples, and update when conventions change. 

  - id: 03-008
    difficulty: senior
    category: "Unknown"
    question: |
      What does the permission pattern 'Bash(git *)' match?
    options:
      - a) Only the exact command 'git'
      - b) Any git command [CORRECT]
      - c) Git commands that start with asterisk
      - d) Git commands with glob patterns
    correct: b
    explanation: |
      The pattern 'Bash(git *)' matches any git command. Permission patterns use wildcards: 'Bash(git *)' matches any git command, 'Bash(pnpm *)' matches any pnpm command, 'mcp__serena__*' matches all Serena MCP tools. The space-based syntax is current - colon syntax like 'Bash(git status:*)' is deprecated.

  - id: 03-009
    difficulty: power
    category: "Unknown"
    question: |
      What is 'Dynamic Memory' (Profile Switching)?
    options:
      - a) Claude's ability to remember across sessions
      - b) Temporarily modifying CLAUDE.md for specific tasks, then restoring [CORRECT]
      - c) Automatic memory compression
      - d) Syncing memory between devices
    correct: b
    explanation: |
      Dynamic Memory means temporarily modifying CLAUDE.md for specific tasks then restoring it. Techniques include: git stash (stash original, modify, restore), profile library (keep profiles like security-audit.md, debugging.md in ~/.claude/profiles/), or parallel instances (different CLAUDE.md in different worktrees). Switch profiles with a script: `claude-profile security-audit`. 

  - id: 03-010
    difficulty: senior
    category: "Unknown"
    question: |
      What happens to files in the .claude/rules/ directory?
    options:
      - a) They must be manually imported
      - b) They are automatically loaded and combined [CORRECT]
      - c) They override CLAUDE.md
      - d) They are ignored unless referenced
    correct: b
    explanation: |
      Files in `.claude/rules/` are automatically loaded and combined. You can create multiple files like code-conventions.md, git-workflow.md, and architecture.md - all are loaded automatically without manual imports. This allows modular organization of project conventions that Claude will follow. 

  - id: 03-011
    difficulty: junior
    category: "Unknown"
    question: |
      Which of these should be gitignored?
    options:
      - a) /project/CLAUDE.md
      - b) .claude/agents/
      - c) .claude/settings.local.json and CLAUDE.local.md [CORRECT]
      - d) .claude/hooks/
    correct: c
    explanation: |
      Files that should be gitignored are: CLAUDE.local.md (local personal instructions) and .claude/settings.local.json (personal permissions). The .claude/CLAUDE.md file is project memory and should be committed. Files to commit include: agents/, commands/, hooks/, skills/, rules/, and settings.json. This separation allows personal customization while sharing team configurations.

  - id: 03-012
    difficulty: power
    category: "Unknown"
    question: |
      What does 'allowedTools' configuration do differently from permission categories?
    options:
      - a) Nothing, they are the same
      - b) Provides granular control with tool-specific patterns in .claude/settings.json [CORRECT]
      - c) Only works for MCP tools
      - d) Requires admin privileges
    correct: b
    explanation: |
      The allowedTools configuration in .claude/settings.json or .claude/settings.local.json provides granular control with specific patterns. For example: 'Read(*)' allows all reads, 'Bash(git status *)' allows git status commands, 'Bash(pnpm *)' allows pnpm commands. You can set progressive permission levels from beginner (very restrictive) to advanced. Never use --dangerously-skip-permissions.

  - id: 03-013
    difficulty: junior
    category: "Unknown"
    question: |
      What is the global config path on macOS/Linux?
    options:
      - a) /etc/claude/
      - b) ~/.claude/ [CORRECT]
      - c) /usr/local/claude/
      - d) ~/.config/claude/
    correct: b
    explanation: |
      On macOS/Linux, the global config path is ~/.claude/. On Windows, it's %USERPROFILE%\.claude\ or C:\Users\YourName\.claude\. This directory contains your global CLAUDE.md, settings, and can include a profiles/ subdirectory for dynamic memory switching. 

  - id: 03-014
    difficulty: senior
    category: "Unknown"
    question: |
      What are the three levels of progressive permission in allowedTools?
    options:
      - a) Admin, User, Guest
      - b) Beginner (very restrictive), Intermediate, Advanced [CORRECT]
      - c) Read, Write, Execute
      - d) Low, Medium, High
    correct: b
    explanation: |
      The three progressive permission levels are: Beginner (very restrictive - only Read, Grep, Glob), Intermediate (adds Bash git/pnpm, TodoRead/Write), and Advanced (adds Edit, Write, WebFetch, Task). Start restrictive and expand as you gain confidence. This prevents accidents while learning. 

  - id: 03-015
    difficulty: power
    category: "Unknown"
    question: |
      Why should you NEVER use --dangerously-skip-permissions?
    options:
      - a) It's deprecated
      - b) It can lead to destructive operations like rm -rf, force push to main, or DROP TABLE [CORRECT]
      - c) It costs more
      - d) It's slower
    correct: b
    explanation: |
      Never use --dangerously-skip-permissions because it can lead to destructive operations. Horror stories include: `rm -rf node_modules` followed by `rm -rf .` (path error), `git push --force` to main unintentionally, `DROP TABLE users` in poorly generated migrations, and deletion of .env files with credentials. Always prefer granular allowedTools instead. 

  - id: 03-016
    difficulty: junior
    category: "Unknown"
    question: |
      What should a global CLAUDE.md contain?
    options:
      - a) Project-specific code conventions
      - b) Personal preferences that apply to all projects (communication style, preferred tools, safety rules) [CORRECT]
      - c) Team member contact information
      - d) Git commit history
    correct: b
    explanation: |
      Global CLAUDE.md (~/.claude/CLAUDE.md) should contain personal preferences that apply to all your projects: communication style (be concise, use code examples), preferred tools (TypeScript over JavaScript, pnpm over npm), and safety rules (always run tests, never force push). Project-specific settings go in project CLAUDE.md. 

  - id: 03-017
    difficulty: senior
    category: "Unknown"
    question: |
      What is stored in settings.json (not settings.local.json)?
    options:
      - a) Personal permissions
      - b) Hook configurations that are committed to the repo [CORRECT]
      - c) API keys
      - d) Cost tracking data
    correct: b
    explanation: |
      settings.json stores hook configurations and is committed to the repo for team sharing. It defines hooks for PreToolUse, PostToolUse, UserPromptSubmit events - specifying matchers and hook scripts. Personal permission overrides go in settings.local.json (gitignored). This separation allows team automation while respecting personal preferences. 

  - id: 03-018
    difficulty: senior
    category: "Unknown"
    question: |
      How many verification domains does the guide recommend for comprehensive quality checks?
    options:
      - a) 3 domains (lint, test, build)
      - b) 8 domains: frontend, backend, types, style, performance, accessibility, security, UX [CORRECT]
      - c) 5 domains: syntax, logic, performance, security, style
      - d) 12 domains covering every possible check
    correct: b
    explanation: |
      Boris Cherny's verification loops recommend 8 domains for comprehensive quality:  1. Frontend (UI renders correctly) 2. Backend (API responds correctly) 3. Types (TypeScript/type checks pass) 4. Style (linting passes) 5. Performance (no regressions) 6. Accessibility (WCAG compliance) 7. Security (no vulnerabilities) 8. UX (user flows work end-to-end)  CLAUDE.md should define which domains to check for each type of change. 

  - id: 03-019
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the 'Fresh Context Pattern' and when should you use it?
    options:
      - a) Clearing browser cache before testing
      - b) Starting a new session with clean context after implementation phase to avoid accumulated noise [CORRECT]
      - c) Resetting git to a clean state
      - d) Deleting the .claude folder to start fresh
    correct: b
    explanation: |
      The Fresh Context Pattern means starting a new Claude Code session between exploration and implementation phases. During exploration, context accumulates noise (dead ends, rejected approaches, debugging output). A fresh session gives Claude clean context focused purely on implementation, improving output quality. 

  - id: 04-001
    difficulty: junior
    category: "Unknown"
    question: |
      What are agents in Claude Code?
    options:
      - a) External software programs
      - b) Specialized AI personas for specific tasks that Claude can delegate to [CORRECT]
      - c) Human assistants
      - d) Automated scripts
    correct: b
    explanation: |
      Agents are specialized sub-processes (AI personas) that Claude can delegate tasks to. They encapsulate domain expertise, like a security reviewer knowing OWASP Top 10 or a backend architect understanding API design. Think of them as "expert consultants" - instead of explaining everything in your prompt, you invoke an agent that already has that expertise. 

  - id: 04-002
    difficulty: junior
    category: "Unknown"
    question: |
      Where should custom agent files be stored?
    options:
      - a) ~/.claude/agents/
      - b) .claude/agents/ [CORRECT]
      - c) /agents/
      - d) .claude/commands/
    correct: b
    explanation: |
      Custom agent files should be stored in `.claude/agents/` within your project directory. They are markdown files with YAML frontmatter. For example: `.claude/agents/backend-architect.md`, `.claude/agents/code-reviewer.md`. These can be committed to version control to share with your team. 

  - id: 04-003
    difficulty: senior
    category: "Unknown"
    question: |
      What fields are REQUIRED in an agent's YAML frontmatter?
    options:
      - a) name, model, tools
      - b) name, description [CORRECT]
      - c) name, description, model, tools
      - d) name, role, skills
    correct: b
    explanation: |
      Only `name` and `description` are required in agent frontmatter. Optional fields include: model (sonnet default, opus, or haiku), tools (comma-separated list), skills (to inherit), and disallowedTools. The description is crucial - it determines when Claude auto-activates the agent, so make it clear and specific. 

  - id: 04-004
    difficulty: junior
    category: "Unknown"
    question: |
      What is the advantage of using agents over direct prompts?
    options:
      - a) Agents are faster
      - b) Agents encapsulate expertise so you don't need to explain everything each time [CORRECT]
      - c) Agents cost less
      - d) Agents can access more files
    correct: b
    explanation: |
      Agents encapsulate expertise. Without agents, you'd write: "Review this code for security issues, focusing on OWASP Top 10, checking for SQL injection, XSS, CSRF..." With agents: "Use the security-reviewer agent to audit this code." The agent already has that expertise, making your prompts shorter and more consistent. 

  - id: 04-005
    difficulty: senior
    category: "Unknown"
    question: |
      Which model should you use for agents performing complex architectural analysis?
    options:
      - a) haiku - for speed
      - b) sonnet - for balance
      - c) opus - for maximum reasoning [CORRECT]
      - d) Any model works equally well
    correct: c
    explanation: |
      Use opus for complex reasoning and architecture tasks. Model selection guidelines: haiku (fast, low cost) for quick tasks and simple changes; sonnet (balanced, default) for most tasks; opus (slow, high cost) for complex reasoning, architecture decisions, and critical security reviews. Match the model to the task's complexity. 

  - id: 04-006
    difficulty: power
    category: "Unknown"
    question: |
      What is 'Tool SEO' in agent design?
    options:
      - a) Making tools searchable online
      - b) Optimizing the description field to improve when Claude auto-activates the agent [CORRECT]
      - c) SEO for documentation
      - d) A marketing technique
    correct: b
    explanation: |
      Tool SEO optimizes the agent's description field to improve auto-activation. Techniques include: using "use PROACTIVELY" to encourage automatic activation, listing explicit trigger keywords, describing specific contexts, and adding short nicknames. A good description: "Security code reviewer - use PROACTIVELY when: reviewing auth code, analyzing API endpoints... Triggers: security, auth, vulnerability, OWASP" 

  - id: 04-007
    difficulty: junior
    category: "Unknown"
    question: |
      What is the best practice for agent design: specialization or generalization?
    options:
      - a) Create one generalist agent that does everything
      - b) Create specialized agents for each domain (security, testing, backend, etc.) [CORRECT]
      - c) Mix specialized and generalist agents
      - d) Avoid agents entirely
    correct: b
    explanation: |
      Always prefer specialization over generalization. Good: separate agents for backend-architect (API, database, performance), security-reviewer (OWASP, auth, encryption), test-engineer (test strategy, coverage, TDD). Bad: one full-stack-expert that "does everything (poorly)". Specialized agents have focused context and domain-specific expertise. 

  - id: 04-008
    difficulty: senior
    category: "Unknown"
    question: |
      How can agents inherit knowledge from skills?
    options:
      - a) By copying skill content into the agent
      - b) Using the 'skills' field in the frontmatter [CORRECT]
      - c) Skills and agents cannot be combined
      - d) By importing skill files
    correct: b
    explanation: |
      Agents inherit skills using the `skills` field in frontmatter. For example: `skills: [security-guardian]`. This composes expertise without duplication - instead of copying OWASP knowledge into every security-related agent, they all inherit from the security-guardian skill. This follows DRY principles for knowledge organization. 

  - id: 04-009
    difficulty: power
    category: "Unknown"
    question: |
      What is the '7-Parallel-Task Method'?
    options:
      - a) Running 7 Claude instances simultaneously
      - b) Launching 7 specialized sub-agents in parallel to implement complete features [CORRECT]
      - c) A debugging technique
      - d) A code review checklist
    correct: b
    explanation: |
      The 7-Parallel-Task Method launches 7 specialized sub-agents in parallel: 1) Components, 2) Styles, 3) Tests, 4) Types, 5) Hooks, 6) Integration, 7) Config. All run in parallel, then results are consolidated. This dramatically speeds up feature implementation by parallelizing independent work streams. 

  - id: 04-010
    difficulty: senior
    category: "Unknown"
    question: |
      What is the recommended agent weight for frequently-used tasks?
    options:
      - a) Heavy (25K+ tokens) for thoroughness
      - b) Medium (10-15K tokens) for balance
      - c) Lightweight (<3K tokens) for speed [CORRECT]
      - d) Weight doesn't matter
    correct: c
    explanation: |
      Use lightweight agents (<3K tokens, <1s init time) for frequent tasks and workers. Golden Rule: "A lightweight agent used 100x > A heavy agent used 10x." Agent weight categories: Lightweight (<3K tokens) for frequent tasks, Medium (10-15K) for analysis/reviews, Heavy (25K+) for architecture/full audits. Match weight to task frequency. 

  - id: 04-011
    difficulty: power
    category: "Unknown"
    question: |
      In multi-agent orchestration, what model combination is recommended?
    options:
      - a) Opus everywhere for quality
      - b) Haiku everywhere for cost savings
      - c) Sonnet orchestrator + Haiku workers + Sonnet validator [CORRECT]
      - d) Use the same model for all agents
    correct: c
    explanation: |
      The recommended pattern is: Sonnet as orchestrator (coordinates), Haiku workers (parallel execution), Sonnet validator (final check). This is 2-2.5x cheaper than using Opus everywhere with equivalent quality for 90% of tasks. For example, refactoring 100 files: Sonnet analyzes and plans, Haiku does parallel edits, Sonnet validates - saving 80-90% cost. 

  - id: 04-012
    difficulty: senior
    category: "Unknown"
    question: |
      What should an agent's output format section include?
    options:
      - a) Only code examples
      - b) Structured deliverables like reports with sections, severity levels, and recommendations [CORRECT]
      - c) Raw text output
      - d) JSON only
    correct: b
    explanation: |
      An agent's output format should specify structured deliverables. For example, a code reviewer agent outputs: Summary, Critical Issues (Must Fix) with file:line references, Warnings (Should Fix), Suggestions (Nice to Have), and Positive Notes. Clear output format ensures consistent, actionable results across all invocations. 

  - id: 04-013
    difficulty: junior
    category: "Unknown"
    question: |
      What is the purpose of the 'disallowedTools' field in agent frontmatter?
    options:
      - a) To list tools the agent must use
      - b) To block specific tools from being used by the agent [CORRECT]
      - c) To disable the agent
      - d) To list deprecated tools
    correct: b
    explanation: |
      The disallowedTools field blocks specific tools from being used by the agent. For example, a code reviewer agent might have `disallowedTools: [WebSearch]` to ensure it focuses on the actual code rather than searching the web. This provides security boundaries and focuses the agent on its core purpose. 

  - id: 04-014
    difficulty: power
    category: "Unknown"
    question: |
      What is the 'Split Role Sub-Agents' pattern?
    options:
      - a) Dividing one agent into multiple files
      - b) Multi-perspective analysis using parallel agents with different expert roles [CORRECT]
      - c) Splitting code review into phases
      - d) Assigning agents to different team members
    correct: b
    explanation: |
      Split Role Sub-Agents provide multi-perspective analysis in parallel. Process: 1) Activate Plan Mode (thinking enabled by default in Opus 4.5), 2) Ask "What expert roles would analyze this?", 3) Select roles (e.g., Security Expert, Senior Dev, Code Reviewer), 4) Run parallel analysis from each perspective, 5) Consolidate reports into recommendations. Great for comprehensive code and UX reviews. 

  - id: 04-015
    difficulty: senior
    category: "Unknown"
    question: |
      When should parallel execution of agents be avoided?
    options:
      - a) When tasks are read-only
      - b) When tasks are destructive (write operations) and dependent on each other [CORRECT]
      - c) When using haiku model
      - d) When more than 3 agents are involved
    correct: b
    explanation: |
      Avoid parallel execution for destructive (write) operations that are dependent on each other. The decision matrix: Independent + Non-destructive = Parallel (max efficiency); Independent + Destructive = Sequential with Plan Mode first; Dependent operations = Sequential (order matters). Parallel writes risk conflicts if files share imports/exports. 

  - id: 04-016
    difficulty: junior
    category: "Unknown"
    question: |
      What does a Debugger agent's methodology typically include?
    options:
      - a) Only fixing code
      - b) Reproduce, Isolate, Analyze, Hypothesize, Test, Fix, Verify [CORRECT]
      - c) Just running tests
      - d) Deleting problematic code
    correct: b
    explanation: |
      A systematic Debugger agent follows: 1) Reproduce - confirm the issue exists, 2) Isolate - narrow down to smallest reproducible case, 3) Analyze - read code, check logs, trace execution, 4) Hypothesize - form theories about cause, 5) Test - verify hypothesis with minimal changes, 6) Fix - implement solution, 7) Verify - confirm fix works without breaking other things. Never guess - always verify. 

  - id: 04-017
    difficulty: senior
    category: "Unknown"
    question: |
      What is an example of a BAD agent description?
    options:
      - a) 'Use when designing APIs, reviewing database schemas, or optimizing backend performance'
      - b) 'Backend stuff' [CORRECT]
      - c) 'Security code reviewer - use PROACTIVELY when reviewing auth code'
      - d) 'Use when encountering errors, test failures, or unexpected behavior'
    correct: b
    explanation: |
      "Backend stuff" is a bad description - it's too vague to help Claude know when to activate the agent. Good descriptions are specific: "Use when designing APIs, reviewing database schemas, or optimizing backend performance" or "Security code reviewer - use PROACTIVELY when reviewing auth code." Clear activation triggers improve agent utilization. 

  - id: 04-018
    difficulty: power
    category: "Unknown"
    question: |
      What is the invocation reliability difference between AGENTS.md (eager context) and Skills (lazy invocation)?
    options:
      - a) Skills are more reliable than AGENTS.md
      - b) Both have 100% reliability
      - c) AGENTS.md: 100% reliable (always loaded) vs Skills: 53-79% auto-invoked [CORRECT]
      - d) AGENTS.md is deprecated in favor of Skills
    correct: c
    explanation: |
      AGENTS.md uses eager context loading - always present in Claude's context, so 100% reliable activation. Skills use lazy invocation - auto-detected and invoked only when Claude recognizes the need, achieving 53-79% auto-invocation rate. Trade-off: AGENTS.md costs context tokens on every session, Skills save tokens but may miss activations. Use AGENTS.md for critical behaviors, Skills for optional capabilities. 

  - id: 05-001
    difficulty: junior
    category: "Unknown"
    question: |
      What is the relationship between Skills and Agents?
    options:
      - a) Skills replace agents
      - b) Skills are knowledge packages that agents can inherit [CORRECT]
      - c) Agents are a type of skill
      - d) They are completely independent
    correct: b
    explanation: |
      Skills are knowledge packages that agents can inherit. While agents are specialized roles (task-focused), skills are reusable knowledge modules (domain-focused). Multiple agents can inherit the same skill, avoiding duplication. For example, both a code-reviewer and security-auditor agent can inherit the security-guardian skill. 

  - id: 05-002
    difficulty: junior
    category: "Unknown"
    question: |
      Where should skill files be stored?
    options:
      - a) .claude/commands/
      - b) .claude/skills/{skill-name}/ [CORRECT]
      - c) .claude/agents/
      - d) ~/.claude/skills/
    correct: b
    explanation: |
      Skills live in `.claude/skills/{skill-name}/` directories within your project. Each skill has its own folder containing at minimum a SKILL.md file, with optional reference.md, checklists/, examples/, and scripts/ subdirectories. This organization keeps knowledge modular and reusable. 

  - id: 05-003
    difficulty: senior
    category: "Unknown"
    question: |
      What is the REQUIRED file in a skill folder?
    options:
      - a) README.md
      - b) skill.yaml
      - c) SKILL.md [CORRECT]
      - d) index.md
    correct: c
    explanation: |
      SKILL.md is the required main file in every skill folder. Optional files include: reference.md (detailed documentation), checklists/ (verification lists), examples/ (code patterns with good/bad examples), and scripts/ (helper scripts). SKILL.md contains the frontmatter and core instructions for the skill. 

  - id: 05-004
    difficulty: junior
    category: "Unknown"
    question: |
      What is the key difference between Skills, Agents, and Commands?
    options:
      - a) They all do the same thing
      - b) Skills = knowledge modules (inherited), Agents = specialized roles (delegated), Commands = process workflows (slash invoked) [CORRECT]
      - c) Commands are the only reusable component
      - d) Agents cannot use skills or commands
    correct: b
    explanation: |
      The three concepts have distinct purposes: Skills are knowledge modules inherited by agents (like OWASP security knowledge), Agents are specialized roles that Claude delegates tasks to (like a security reviewer), and Commands are process workflows invoked with slash commands (like /tech:commit). They can be combined: agents inherit skills and execute commands. 

  - id: 05-005
    difficulty: senior
    category: "Unknown"
    question: |
      What does the 'context' field in SKILL.md frontmatter control?
    options:
      - a) The context window size
      - b) Whether the skill runs in isolated (fork) or shared (inherit) context [CORRECT]
      - c) The file context to load
      - d) Database connection context
    correct: b
    explanation: |
      The `context` field controls execution context: 'fork' means isolated context (the skill runs independently), 'inherit' means shared context (the skill shares context with the calling agent). Use fork for skills that need clean state, inherit for skills that need access to conversation history and loaded files. 

  - id: 05-006
    difficulty: junior
    category: "Unknown"
    question: |
      Why use skills instead of duplicating knowledge in multiple agents?
    options:
      - a) Skills are faster
      - b) Skills provide single source of truth - update once, all agents benefit [CORRECT]
      - c) Skills are required by Claude Code
      - d) Skills cost less
    correct: b
    explanation: |
      Skills follow DRY (Don't Repeat Yourself) principles. Without skills, you'd duplicate security knowledge in Agent A, B, and C. With skills, the security-guardian skill is the single source - all agents inherit it, and updates propagate everywhere. This ensures consistency and simplifies maintenance. 

  - id: 05-007
    difficulty: senior
    category: "Unknown"
    question: |
      What makes a GOOD skill versus a BAD skill?
    options:
      - a) Good skills are longer, bad skills are shorter
      - b) Good skills are reusable, domain-focused, and include reference material; bad skills are single-agent specific and too broad [CORRECT]
      - c) Good skills use opus, bad skills use haiku
      - d) Good skills have more files
    correct: b
    explanation: |
      Good skills are: reusable across multiple agents, domain-focused (not too broad), contain reference material and checklists, and include verification steps. Bad skills are: specific to only one agent, too broad in scope, just instructions without reference material, and missing verification checklists. 

  - id: 05-008
    difficulty: power
    category: "Unknown"
    question: |
      What is the TDD (Test-Driven Development) skill's core methodology?
    options:
      - a) Write tests after code
      - b) RED (failing test) -> GREEN (minimal code to pass) -> REFACTOR (improve while green) [CORRECT]
      - c) Write all tests first, then all code
      - d) Skip tests for speed
    correct: b
    explanation: |
      The TDD skill follows: 1) RED - write a failing test for desired behavior BEFORE code, 2) GREEN - write MINIMUM code to make the test pass, 3) REFACTOR - improve implementation while keeping tests green, then repeat. This cycle ensures tests actually verify behavior by requiring failure first, then incremental improvement. 

  - id: 05-009
    difficulty: senior
    category: "Unknown"
    question: |
      What should a Security Guardian skill include for OWASP coverage?
    options:
      - a) Just a list of vulnerabilities
      - b) Checklists for each OWASP Top 10 category with specific verification items [CORRECT]
      - c) Links to external security tools
      - d) Password examples
    correct: b
    explanation: |
      A Security Guardian skill should include detailed checklists for each OWASP Top 10 category: A01 Broken Access Control (check authorization, IDOR, privilege escalation), A02 Cryptographic Failures (hardcoded secrets, TLS, password hashing), A03 Injection (SQL, NoSQL, XSS), etc. Each category should have specific verification items with checkboxes. 

  - id: 05-010
    difficulty: junior
    category: "Unknown"
    question: |
      What is the 'agent' field in SKILL.md frontmatter?
    options:
      - a) Which agent created the skill
      - b) Whether the skill is 'specialist' (domain-focused) or 'general' (broad) [CORRECT]
      - c) The agent that must use this skill
      - d) The agent's name
    correct: b
    explanation: |
      The `agent` field indicates whether the skill is 'specialist' (domain-focused, deep expertise in one area) or 'general' (broad, applicable across domains). Specialist skills like security-guardian provide deep domain knowledge, while general skills might provide widely-applicable patterns or utilities. 

  - id: 05-011
    difficulty: power
    category: "Unknown"
    question: |
      What does the 'allowed-tools' field in skill frontmatter control?
    options:
      - a) Tools the skill documents
      - b) Tools the skill can use when activated [CORRECT]
      - c) Tools that can activate the skill
      - d) Tools to install
    correct: b
    explanation: |
      The `allowed-tools` field specifies which tools the skill can use when activated. For example, a security-guardian skill might have `allowed-tools: Read, Grep, Bash` - allowing it to read files, search for patterns, and run security scanning commands, but not modify files. This provides security boundaries for each skill. 

  - id: 05-012
    difficulty: senior
    category: "Unknown"
    question: |
      What optional folders can a skill directory contain?
    options:
      - a) Only SKILL.md is allowed
      - b) reference.md, checklists/, examples/, and scripts/ [CORRECT]
      - c) src/, dist/, and node_modules/
      - d) tests/, docs/, and config/
    correct: b
    explanation: |
      A skill directory can contain: SKILL.md (required), reference.md (detailed documentation), checklists/ (verification lists like security.md, performance.md), examples/ (code patterns like good-example.ts, bad-example.ts), and scripts/ (helper scripts like audit.sh). This rich structure supports comprehensive domain knowledge. 

  - id: 05-013
    difficulty: junior
    category: "Unknown"
    question: |
      In the Security Guardian skill, what is an example of GOOD password hashing?
    options:
      - a) md5(password)
      - b) sha1(password)
      - c) argon2 or bcrypt [CORRECT]
      - d) Base64 encoding
    correct: c
    explanation: |
      The Security Guardian skill shows argon2 or bcrypt as secure password hashing. BAD examples explicitly listed: md5(password) and sha1(password) - these are cryptographically broken for password storage. Good pattern: `const hashedPassword = await hash(password)` using argon2 library. Always verify password with `await verify(hashedPassword, inputPassword)`. 

  - id: 05-014
    difficulty: senior
    category: "Unknown"
    question: |
      What is the AAA pattern in TDD testing?
    options:
      - a) Ask, Answer, Assert
      - b) Arrange, Act, Assert [CORRECT]
      - c) Analyze, Apply, Approve
      - d) Accept, Adjust, Acknowledge
    correct: b
    explanation: |
      AAA stands for Arrange, Act, Assert: 1) Arrange - set up test data and preconditions, 2) Act - execute the code being tested, 3) Assert - verify the result matches expectations. Example: Arrange items array, Act by calling calculateTotal(items), Assert that total equals expected value. This structure makes tests readable and maintainable. 

  - id: 05-015
    difficulty: power
    category: "Unknown"
    question: |
      What community skill repository focuses on cybersecurity and penetration testing?
    options:
      - a) awesome-claude-skills
      - b) claude-security-pack
      - c) zebbern/claude-code-guide with 29 cybersecurity skills [CORRECT]
      - d) owasp-claude-skills
    correct: c
    explanation: |
      The zebbern/claude-code-guide repository contains 29 cybersecurity-focused skills covering: penetration testing (SQL injection, XSS, IDOR), security tools (Metasploit, Burp Suite, SQLMap), infrastructure security (AWS, Cloud, Network), and methodologies (ethical hacking, pentest checklists). Important: these should be tested thoroughly and used only with proper authorization. 

  - id: 05-016
    difficulty: junior
    category: "Unknown"
    question: |
      What should skills include to be most useful?
    options:
      - a) Only theory and concepts
      - b) Checklists, good/bad code examples, and methodology steps [CORRECT]
      - c) Links to external websites
      - d) Marketing descriptions
    correct: b
    explanation: |
      Useful skills include: methodology steps (clear process to follow), checklists (verification items with checkboxes), good/bad code examples (showing correct patterns and anti-patterns), and reference material. This combination provides both theoretical knowledge and practical guidance that agents can apply directly to tasks. 

  - id: 05-017
    difficulty: senior
    category: "Unknown"
    question: |
      How do you reference a skill in an agent's frontmatter?
    options:
      - a) import: skill-name
      - b) skills: [skill-name] [CORRECT]
      - c) use: skill-name
      - d) require: skill-name
    correct: b
    explanation: |
      Reference skills in an agent's frontmatter using the `skills` array: `skills: [security-guardian, tdd]`. This makes the agent inherit all knowledge from those skills. You can reference multiple skills, and the agent combines their expertise. The skill name matches the folder name in `.claude/skills/`. 

  - id: 05-018
    difficulty: power
    category: "Unknown"
    question: |
      What disclaimer applies to community cybersecurity skills?
    options:
      - a) They are officially certified
      - b) Test thoroughly, ensure authorization, verify against policies, use only legally [CORRECT]
      - c) They work on all systems
      - d) No disclaimer needed
    correct: b
    explanation: |
      Community cybersecurity skills come with important disclaimers: test thoroughly before using in production assessments, ensure you have proper authorization before penetration testing, review and validate against your organization's security policies, use only in legal contexts with written permission from system owners, and contribute back if you find issues. Verification is essential for any security tooling. 

  - id: 06-001
    difficulty: junior
    category: "Unknown"
    question: |
      Where should custom commands be placed to make them available in Claude Code?
    options:
      - a) ~/.claude/commands/
      - b) .claude/commands/ [CORRECT]
      - c) /usr/local/claude/commands/
      - d) .claude/config/commands/
    correct: b
    explanation: |
      Custom commands are placed in `.claude/commands/` within your project directory.  This allows project-specific commands that can be committed with your codebase. The global directory `~/.claude/` is for personal settings, not project commands.  Commands are organized in subdirectories: - `.claude/commands/tech/` for development workflows - `.claude/commands/product/` for product workflows 

  - id: 06-002
    difficulty: junior
    category: "Unknown"
    question: |
      How do you invoke a custom command named `commit.md` located in `.claude/commands/tech/`?
    options:
      - a) /commit
      - b) /tech-commit
      - c) /tech:commit [CORRECT]
      - d) !tech/commit
    correct: c
    explanation: |
      Custom commands use the format `/folder:filename` (without the .md extension).  So `.claude/commands/tech/commit.md` becomes `/tech:commit`. This naming convention allows organizing commands by domain while keeping invocation intuitive.  Examples: - `.claude/commands/tech/pr.md` -> `/tech:pr` - `.claude/commands/product/scope.md` -> `/product:scope` 

  - id: 06-003
    difficulty: senior
    category: "Unknown"
    question: |
      What variable is used in command files to access arguments passed to the command?
    options:
      - a) $ARGS
      - b) $INPUT
      - c) $ARGUMENTS [CORRECT]
      - d) $PARAMS
    correct: c
    explanation: |
      The `$ARGUMENTS` variable contains any text passed after the command invocation.  For example, when you run `/tech:deploy production`, the variable `$ARGUMENTS` will contain `production`.  This enables dynamic commands that can adapt based on user input. Commands should document how they handle arguments and what happens if none are provided. 

  - id: 06-004
    difficulty: junior
    category: "Unknown"
    question: |
      Which built-in command shows all available commands including custom ones?
    options:
      - a) /commands
      - b) /list
      - c) /help [CORRECT]
      - d) /show
    correct: c
    explanation: |
      The `/help` command displays all available commands, both built-in and custom.  Built-in commands include: - `/clear` - Clear conversation - `/compact` - Summarize context - `/status` - Show session info - `/plan` - Enter Plan Mode - `/rewind` - Undo changes  Custom commands from `.claude/commands/` are also listed here. 

  - id: 06-005
    difficulty: senior
    category: "Unknown"
    question: |
      What sections should a well-structured command template include according to best practices?
    options:
      - a) Purpose, Steps, Output
      - b) Purpose, Process, Arguments, Output Format, Examples, Error Handling [CORRECT]
      - c) Name, Description, Code
      - d) Title, Body, Footer
    correct: b
    explanation: |
      A complete command template should include:  1. **Purpose** - Brief description of what the command does 2. **Process** - Step-by-step instructions Claude should follow 3. **Arguments** - How to handle $ARGUMENTS (if provided/not provided) 4. **Output Format** - Expected structure of the output 5. **Examples** - Concrete usage examples 6. **Error Handling** - How to handle edge cases and failures  This comprehensive structure ensures consistent, reliable command execution. 

  - id: 06-006
    difficulty: junior
    category: "Unknown"
    question: |
      Which command enters Plan Mode for safe, read-only exploration?
    options:
      - a) /safe
      - b) /readonly
      - c) /plan [CORRECT]
      - d) /explore
    correct: c
    explanation: |
      The `/plan` command enters Plan Mode, where Claude can analyze and explore the codebase without making any changes.  This is ideal for: - Understanding unfamiliar codebases - Architectural analysis before changes - Safe exploration of risky operations  Use `/execute` to exit Plan Mode when ready to make changes. 

  - id: 06-007
    difficulty: senior
    category: "Unknown"
    question: |
      In the commit command example, what is the recommended commit message format?
    options:
      - a) Simple description
      - b) Conventional Commits: type(scope): description [CORRECT]
      - c) Date - Author - Message
      - d) JIRA-123: Message
    correct: b
    explanation: |
      The guide recommends Conventional Commits format: `type(scope): description`  Common types: - `feat`: New feature - `fix`: Bug fix - `refactor`: Code restructuring - `docs`: Documentation - `test`: Test changes - `chore`: Maintenance  This provides consistent, parseable commit history useful for changelogs and releases. 

  - id: 06-008
    difficulty: power
    category: "Unknown"
    question: |
      What technique does the Problem Framer command use to find root causes?
    options:
      - a) SWOT Analysis
      - b) 5 Whys Analysis [CORRECT]
      - c) Pareto Analysis
      - d) Fishbone Diagram
    correct: b
    explanation: |
      The Problem Framer command uses the "5 Whys Analysis" technique.  This involves asking "Why?" five times to drill down to the root cause: - Why 1: First answer - Why 2: Deeper answer - Why 3: Even deeper - Why 4: Getting to root - Why 5: Root cause  The command then reframes the problem as: "How might we [action] for [user] so that [outcome]?" 

  - id: 06-009
    difficulty: junior
    category: "Unknown"
    question: |
      What does the `/rewind` command do?
    options:
      - a) Restores a previous git commit
      - b) Undoes Claude's recent changes in the current session [CORRECT]
      - c) Clears the entire conversation history
      - d) Rolls back the last command execution
    correct: b
    explanation: |
      The `/rewind` command undoes Claude's recent changes in the current session.  Key points: - Works only for uncommitted changes made by Claude - Does NOT create git commits - Use when Claude made a mistake and you want to try a different approach  For committed changes, use `git revert` instead. The keyboard shortcut `Esc×2` (double-tap Escape) also triggers rewind. 

  - id: 06-010
    difficulty: senior
    category: "Unknown"
    question: |
      What should a PR command's error handling do if the user is NOT on a feature branch?
    options:
      - a) Automatically create a feature branch
      - b) Proceed anyway with warnings
      - c) WARN: Create a feature branch first [CORRECT]
      - d) Exit silently
    correct: c
    explanation: |
      According to the PR command example, if the user is not on a feature branch, the command should WARN: "Create a feature branch first".  Similarly, if the working directory is dirty, it should ASK: "Commit changes first?"  Good command design includes clear error handling that: - Warns users about prerequisites - Suggests corrective actions - Prevents accidental mistakes 

  - id: 06-011
    difficulty: power
    category: "Unknown"
    question: |
      What is the recommended order of git commands in a PR creation workflow?
    options:
      - a) push, diff, create PR
      - b) status, branch, log, diff, push if needed, create PR [CORRECT]
      - c) add, commit, push, create PR
      - d) checkout, status, push, create PR
    correct: b
    explanation: |
      The recommended PR workflow order is:  1. `git status` - Verify clean working directory 2. `git branch` - Confirm on feature branch 3. `git log main..HEAD` - Review all commits 4. `git diff main...HEAD` - See all changes vs main 5. `git push -u origin [branch]` - Push if not already pushed 6. `gh pr create` - Create the PR  This thorough process ensures quality PRs with proper context. 

  - id: 06-012
    difficulty: junior
    category: "Unknown"
    question: |
      How do you reference a file when talking to Claude Code?
    options:
      - a) #filename
      - b) @filename [CORRECT]
      - c) !filename
      - d) $filename
    correct: b
    explanation: |
      The `@filename` syntax references a file in your conversation with Claude.  Quick actions: - `@file` - Reference a file - `!command` - Run a shell command - `Ctrl+C` - Cancel operation - `Ctrl+R` - Retry last  This allows you to easily bring specific files into context for Claude to analyze. 

  - id: 07-001
    difficulty: junior
    category: "Unknown"
    question: |
      What exit code should a hook return to BLOCK an operation?
    options:
      - a) 0
      - b) 1
      - c) 2 [CORRECT]
      - d) 255
    correct: c
    explanation: |
      Hook exit codes have specific meanings:  - **Exit code 0**: Success - Allow the operation to proceed - **Exit code 2**: Block - Prevent the operation from executing - **Other codes**: Error - Log the error and continue  This is critical for security hooks that need to prevent dangerous commands. For example, a hook that detects `rm -rf /` should `exit 2` to block execution. 

  - id: 07-002
    difficulty: junior
    category: "Unknown"
    question: |
      Which hook event fires BEFORE a tool is executed?
    options:
      - a) BeforeToolUse
      - b) PreToolUse [CORRECT]
      - c) ToolStart
      - d) OnToolCall
    correct: b
    explanation: |
      The `PreToolUse` event fires before any tool runs.  Common event types: - **PreToolUse**: Before tool execution (ideal for security validation) - **PostToolUse**: After tool execution (for formatting, logging) - **UserPromptSubmit**: When user sends a message (context enrichment) - **Notification**: When Claude sends a notification - **SessionStart/SessionEnd**: Session lifecycle events - **Stop**: When user interrupts 

  - id: 07-003
    difficulty: senior
    category: "Unknown"
    question: |
      How do hooks receive input data from Claude Code?
    options:
      - a) As command-line arguments
      - b) As JSON on stdin [CORRECT]
      - c) As environment variables
      - d) From a temporary file
    correct: b
    explanation: |
      Hooks receive JSON data on stdin with information about the event.  Example input structure: ```json {   "tool_name": "Bash",   "tool_input": {     "command": "git status"   },   "session_id": "abc123",   "cwd": "/project" } ```  Hooks typically parse this with: `INPUT=$(cat)` followed by `jq` for JSON extraction. 

  - id: 07-004
    difficulty: senior
    category: "Unknown"
    question: |
      In the hook registration (settings.json), what does the `matcher` field specify?
    options:
      - a) File extensions to watch
      - b) Regex pattern for which tools trigger the hook [CORRECT]
      - c) User permission levels
      - d) Output format requirements
    correct: b
    explanation: |
      The `matcher` field is a regex pattern that determines which tools trigger the hook.  Example configuration: ```json {   "matcher": "Bash|Edit|Write",   "hooks": [{"type": "command", "command": "./hooks/security-check.sh"}] } ```  This hook would trigger for Bash, Edit, or Write tools. You can match specific tools or use `.*` to match all tools. 

  - id: 07-005
    difficulty: power
    category: "Unknown"
    question: |
      What is the best use case for the `UserPromptSubmit` hook event?
    options:
      - a) Blocking dangerous commands
      - b) Auto-formatting code
      - c) Adding context like git status to every prompt [CORRECT]
      - d) Playing notification sounds
    correct: c
    explanation: |
      The `UserPromptSubmit` event is ideal for context enrichment.  Use cases: - **UserPromptSubmit**: Add context (git status, current branch, staged files) - **PreToolUse**: Security validation (block dangerous commands) - **PostToolUse**: Formatting, logging, quality checks - **Notification**: Sound alerts, desktop notifications  The context enricher example adds git branch, last commit, and staged/unstaged info. 

  - id: 07-006
    difficulty: junior
    category: "Unknown"
    question: |
      What exit code allows an operation to proceed?
    options:
      - a) 1
      - b) 0 [CORRECT]
      - c) 2
      - d) -1
    correct: b
    explanation: |
      Exit code 0 means success and allows the operation to proceed.  The exit code system: - **0**: Success - Allow operation - **2**: Block - Prevent operation - **Other**: Error - Log and continue  Always end your hooks with `exit 0` if you want to allow the operation, or `exit 2` to block it. 

  - id: 07-007
    difficulty: senior
    category: "Unknown"
    question: |
      Which commands should a security hook typically block?
    options:
      - a) git status, npm test
      - b) rm -rf /, sudo rm, git push --force origin main [CORRECT]
      - c) cd, ls, pwd
      - d) npm install, pip install
    correct: b
    explanation: |
      Security hooks should block dangerous operations like:  - `rm -rf /` or `rm -rf ~` - Filesystem destruction - `sudo rm` - Privileged deletion - `git push --force origin main` - Force push to protected branches - `npm publish` - Accidental package publishing - `> /dev/sda` or `dd if=` - Direct disk operations  Safe commands like `git status`, `npm test`, `ls` should be allowed. 

  - id: 07-008
    difficulty: power
    category: "Unknown"
    question: |
      What JSON structure should a hook return to send a message back to Claude?
    options:
      - a) {"message": "text"}
      - b) {"systemMessage": "text", "hookSpecificOutput": {...}} [CORRECT]
      - c) {"response": "text"}
      - d) {"output": "text"}
    correct: b
    explanation: |
      Hooks return JSON on stdout with specific fields:  ```json {   "systemMessage": "Message shown to Claude",   "hookSpecificOutput": {     "additionalContext": "Extra information"   } } ```  - `systemMessage`: Displayed to Claude as context - `hookSpecificOutput`: Additional structured data  This allows hooks to provide context that Claude can use in its responses. 

  - id: 07-009
    difficulty: senior
    category: "Unknown"
    question: |
      What is the recommended approach for tasks that need 'understanding' vs pattern-based tasks?
    options:
      - a) Both should use bash scripts
      - b) Both should use AI agents
      - c) Pattern-based use bash scripts; understanding-needed use AI agents [CORRECT]
      - d) Pattern-based use AI agents; understanding-needed use bash scripts
    correct: c
    explanation: |
      The guide recommends choosing the right tool:  **Use Bash scripts when:** - Tasks are deterministic (create branch, push) - Pattern-based (check for secrets with regex) - Fast, predictable, no token cost  **Use AI Agents when:** - Interpretation is needed (code review quality) - Context-dependent decisions - Understanding and judgment required  Rule: "If you can write a regex for it, use a bash script." 

  - id: 07-010
    difficulty: junior
    category: "Unknown"
    question: |
      Which hook event fires AFTER a tool has finished executing?
    options:
      - a) AfterToolUse
      - b) PostToolUse [CORRECT]
      - c) ToolComplete
      - d) OnToolDone
    correct: b
    explanation: |
      The `PostToolUse` event fires after any tool completes execution.  Common use cases for PostToolUse: - Auto-formatting code after edits - Running linters after file changes - Logging tool usage for auditing - Triggering tests after code changes  The Auto-Formatter template uses PostToolUse to run Prettier after Edit/Write operations. 

  - id: 07-011
    difficulty: power
    category: "Unknown"
    question: |
      How do you test a security hook before deploying it?
    options:
      - a) Run Claude Code and hope it works
      - b) Pipe test JSON to the hook script and check the exit code [CORRECT]
      - c) Deploy to production and monitor
      - d) Security hooks cannot be tested
    correct: b
    explanation: |
      Test hooks by piping JSON input and checking the exit code:  ```bash # Test with a blocked command echo '{"tool_name":"Bash","tool_input":{"command":"rm -rf /"}}' | ./hooks/security-blocker.sh echo "Exit code: $?"  # Should be 2  # Test with a safe command echo '{"tool_name":"Bash","tool_input":{"command":"git status"}}' | ./hooks/security-blocker.sh echo "Exit code: $?"  # Should be 0 ```  This ensures your hook correctly blocks dangerous commands before deployment. 

  - id: 07-012
    difficulty: senior
    category: "Unknown"
    question: |
      In Windows, how should hooks be invoked to avoid execution policy restrictions?
    options:
      - a) Run as Administrator
      - b) Use powershell -ExecutionPolicy Bypass -File script.ps1 [CORRECT]
      - c) Disable all security settings
      - d) Convert to batch files only
    correct: b
    explanation: |
      Windows hooks should use the full PowerShell invocation:  ```json {   "type": "command",   "command": "powershell -ExecutionPolicy Bypass -File .claude/hooks/security-check.ps1" } ```  This bypasses the default execution policy that might block script execution. Batch files (.cmd) can also be used as an alternative for simpler hooks. 

  - id: 07-013
    difficulty: power
    category: "Unknown"
    question: |
      What does the activity logger hook example use to store logs?
    options:
      - a) SQLite database
      - b) Plain text files
      - c) JSONL (JSON Lines) files [CORRECT]
      - d) CSV files
    correct: c
    explanation: |
      The activity logger hook stores logs in JSONL format (JSON Lines).  Key features: - Logs to `~/.claude/logs/activity-YYYY-MM-DD.jsonl` - Each entry contains timestamp, tool name, and session ID - Auto-cleanup of logs older than 7 days - Uses `jq` for JSON construction  JSONL is ideal for log files as each line is a valid JSON object, making it easy to append and parse. 

  - id: 07-014
    difficulty: senior
    category: "Unknown"
    question: |
      What is the default timeout for hooks in the configuration?
    options:
      - a) 1000ms (1 second)
      - b) 5000ms (5 seconds) [CORRECT]
      - c) 30000ms (30 seconds)
      - d) No timeout by default
    correct: b
    explanation: |
      The example hook configuration shows a timeout of 5000ms (5 seconds).  ```json {   "type": "command",   "command": ".claude/hooks/security-check.sh",   "timeout": 5000 } ```  This prevents hooks from blocking Claude Code indefinitely. For longer operations like formatting, you might increase this to 10000ms. 

  - id: 07-015
    difficulty: senior
    category: "Unknown"
    question: |
      What configuration parameter makes a hook run asynchronously (v2.1.0+)?
    options:
      - a) background: true
      - b) async: true [CORRECT]
      - c) nonblocking: true
      - d) mode: async
    correct: b
    explanation: |
      Since Claude Code v2.1.0, hooks support `async: true` in their configuration. This makes the hook fire without blocking Claude's execution. Useful for logging, notifications, or analytics hooks where you don't need to wait for the result before continuing. 

  - id: 07-016
    difficulty: power
    category: "Unknown"
    question: |
      What are the 3 limitations of async hooks compared to sync hooks?
    options:
      - a) Can't access stdin, can't write files, can't read environment variables
      - b) No exit code feedback (can't block), no additionalContext returned, no blocking capability [CORRECT]
      - c) Can't use network, can't spawn processes, can't read files
      - d) Limited to 1s timeout, no stderr capture, no argument passing
    correct: b
    explanation: |
      Async hooks trade control for speed:  1. **No exit code feedback** - Can't block Claude based on success/failure 2. **No additionalContext** - Can't inject context back into the conversation 3. **No blocking capability** - Fire-and-forget only  Use async for non-critical operations (logging, telemetry). Use sync for security gates, formatting, and validation. 

  - id: 08-001
    difficulty: junior
    category: "Unknown"
    question: |
      What does MCP stand for in the context of Claude Code?
    options:
      - a) Model Compute Protocol
      - b) Model Context Protocol [CORRECT]
      - c) Multi-Channel Processing
      - d) Module Configuration Protocol
    correct: b
    explanation: |
      MCP stands for Model Context Protocol.  It is a standard for connecting AI models to external tools and data sources. MCP enables Claude Code to extend beyond built-in tools by connecting to: - Semantic code analysis (Serena) - Documentation lookup (Context7) - Database queries (Postgres) - Browser automation (Playwright) 

  - id: 08-002
    difficulty: senior
    category: "Unknown"
    question: |
      Which MCP server should you use to find all usages of a function across your codebase?
    options:
      - a) Context7
      - b) Sequential Thinking
      - c) Serena [CORRECT]
      - d) Postgres
    correct: c
    explanation: |
      Serena provides semantic code analysis with tools like `find_referencing_symbols`.  Serena tools include: - `find_symbol` - Find functions, classes, methods by name - `get_symbols_overview` - Get file structure overview - `find_referencing_symbols` - Find all usages of a symbol - `search_for_pattern` - Regex search across codebase  Use Serena for deep code understanding and symbol-level analysis. 

  - id: 08-003
    difficulty: senior
    category: "Unknown"
    question: |
      Which MCP server is best for looking up official library documentation?
    options:
      - a) Serena
      - b) Context7 [CORRECT]
      - c) Sequential Thinking
      - d) mgrep
    correct: b
    explanation: |
      Context7 is designed for accessing official library documentation.  Use Context7 when: - Learning new libraries - Finding correct API usage - Checking official patterns  For example, "How does React useEffect work?" should use Context7 to get the official documentation rather than generic knowledge. 

  - id: 08-004
    difficulty: power
    category: "Unknown"
    question: |
      Which MCP server provides persistent memory across sessions?
    options:
      - a) Context7
      - b) Postgres
      - c) Serena [CORRECT]
      - d) Sequential Thinking
    correct: c
    explanation: |
      Serena provides session memory that persists across conversations.  Serena memory tools: - `write_memory` - Save context for future sessions - `read_memory` - Retrieve saved context - `list_memories` - List all stored memories  Memory is stored in `.serena/memories/` and survives between Claude Code sessions. This is crucial for maintaining context on long-running projects. 

  - id: 08-005
    difficulty: junior
    category: "Unknown"
    question: |
      Where is the global MCP configuration file located?
    options:
      - a) ~/.mcp/config.json
      - b) ~/.claude/mcp.json [CORRECT]
      - c) /etc/claude/mcp.json
      - d) ~/.config/claude/mcp.json
    correct: b
    explanation: |
      The global MCP configuration is at `~/.claude/mcp.json`.  Configuration locations: - `~/.claude/mcp.json` - Global (applies to all projects) - `/project/.claude/mcp.json` - Project-specific (overrides global)  The configuration specifies which servers to run and their settings. 

  - id: 08-006
    difficulty: senior
    category: "Unknown"
    question: |
      What is the recommended MCP server for complex debugging scenarios?
    options:
      - a) Context7
      - b) Serena
      - c) Sequential Thinking [CORRECT]
      - d) Postgres
    correct: c
    explanation: |
      Sequential Thinking is designed for multi-step analysis with explicit reasoning.  Use Sequential Thinking for: - Complex debugging scenarios - Architectural analysis - System design decisions  The `sequentialthinking` tool provides step-by-step reasoning that is ideal for problems requiring systematic investigation. 

  - id: 08-007
    difficulty: power
    category: "Unknown"
    question: |
      How can MCP servers work together effectively?
    options:
      - a) They cannot - only one server at a time
      - b) Context7 for patterns -> Serena for code -> Sequential for analysis -> Playwright for testing [CORRECT]
      - c) All servers must be configured identically
      - d) Servers must be chained in alphabetical order
    correct: b
    explanation: |
      MCP servers can be combined for powerful workflows:  1. **Context7** - Get official pattern for auth 2. **Serena** - Find existing auth code in codebase 3. **Sequential** - Analyze how to integrate 4. **Playwright** - Test the implementation  This combination leverages each server's strengths for comprehensive development. 

  - id: 08-008
    difficulty: junior
    category: "Unknown"
    question: |
      What variable can you use in mcp.json to reference the current project path?
    options:
      - a) ${projectPath}
      - b) ${workspaceFolder} [CORRECT]
      - c) ${cwd}
      - d) ${PROJECT_DIR}
    correct: b
    explanation: |
      The `${workspaceFolder}` variable expands to the current project path.  Variable substitution in mcp.json: - `${workspaceFolder}` - Current project path - `${env:VAR_NAME}` - Environment variable  Example: ```json "env": {   "PROJECT_PATH": "${workspaceFolder}" } ``` 

  - id: 08-009
    difficulty: senior
    category: "Unknown"
    question: |
      What is the key advantage of Serena over Claude Code's built-in tools?
    options:
      - a) It's faster
      - b) It provides indexation that Claude Code lacks [CORRECT]
      - c) It uses less memory
      - d) It works offline
    correct: b
    explanation: |
      Serena fills a critical gap: Claude Code has no built-in indexation (unlike Cursor).  Serena provides: - **Indexation**: Pre-indexes your codebase for efficient symbol lookup - **Project Memory**: Stores context between sessions - **Onboarding**: Auto-analyzes project structure on first run  For large codebases (>10k lines), Serena's indexation dramatically improves performance. 

  - id: 08-010
    difficulty: power
    category: "Unknown"
    question: |
      What distinguishes a Plugin from an MCP Server in Claude Code?
    options:
      - a) Plugins are faster
      - b) Plugins bundle Claude-specific workflows; MCP servers add external tool capabilities [CORRECT]
      - c) They are identical
      - d) MCP servers are for beginners, plugins for experts
    correct: b
    explanation: |
      The rule of thumb: - **Plugin** = "How Claude thinks" (new workflows, specialized agents) - **MCP Server** = "What Claude can do" (new tools, external systems)  Plugins bundle agents, skills, and configuration into installable modules. MCP servers add external capabilities like database access or browser automation.  Installation differs too: - Plugins: `claude plugin install` - MCP: Add to `settings.json` MCP config 

  - id: 08-011
    difficulty: junior
    category: "Unknown"
    question: |
      Which MCP server is used for browser automation and E2E testing?
    options:
      - a) Serena
      - b) Context7
      - c) Playwright [CORRECT]
      - d) Sequential Thinking
    correct: c
    explanation: |
      Playwright MCP provides browser automation capabilities.  Playwright tools include: - `navigate` - Go to URL - `click` - Click element - `fill` - Fill form field - `screenshot` - Capture screenshot  Use Playwright for: - E2E testing - Visual validation - Browser debugging 

  - id: 08-012
    difficulty: senior
    category: "Unknown"
    question: |
      How do you add an MCP server with environment variables via CLI?
    options:
      - a) claude mcp add --env API_KEY=key server
      - b) claude mcp add -e API_KEY=key my-server -- npx @org/server [CORRECT]
      - c) claude mcp install server --api-key key
      - d) claude add mcp server -k key
    correct: b
    explanation: |
      Use the `-e` flag to pass environment variables:  ```bash claude mcp add -e API_KEY=your-key my-server -- npx @org/server ```  For multiple environment variables: ```bash claude mcp add -e DATABASE_URL=postgres://... -e DEBUG=true postgres -- npx @prisma/postgres ```  This is quicker than manually editing mcp.json. 

  - id: 08-013
    difficulty: power
    category: "Unknown"
    question: |
      What is grepai's key differentiator compared to Serena?
    options:
      - a) It's faster
      - b) It provides semantic search by intent plus call graph analysis [CORRECT]
      - c) It supports more languages
      - d) It has better documentation
    correct: b
    explanation: |
      grepai excels at intent-based search using natural language, plus offers call graph analysis to trace function dependencies:  ```bash # Semantic search (finds code by meaning, not exact text) grepai search "user authentication flow"  # Who calls this function? grepai trace callers "createSession" ```  While Serena focuses on symbol-level analysis, grepai finds code by describing what it does and traces caller/callee relationships.  Use grepai for exploring unfamiliar codebases or understanding dependencies. 

  - id: 08-014
    difficulty: senior
    category: "Unknown"
    question: |
      What command lists all installed plugins in Claude Code?
    options:
      - a) claude plugins list
      - b) claude plugin [CORRECT]
      - c) /plugins
      - d) claude list plugins
    correct: b
    explanation: |
      Running `claude plugin` (without subcommand) lists all installed plugins with status.  Plugin commands: - `claude plugin` - List installed plugins - `claude plugin install <name>` - Install plugin - `claude plugin enable <name>` - Enable plugin - `claude plugin disable <name>` - Disable plugin - `claude plugin uninstall <name>` - Remove plugin - `claude plugin validate <path>` - Validate plugin manifest 

  - id: 08-015
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the main advantage of using Figma MCP over screenshots?
    options:
      - a) Higher image quality
      - b) 3-10x fewer tokens: structured data vs. image analysis [CORRECT]
      - c) Faster download speed
      - d) Works offline
    correct: b
    explanation: |
      Figma MCP provides 3-10x fewer tokens than screenshots because it returns structured data (React+Tailwind structure, design tokens) instead of requiring image analysis. Other benefits: direct token access, component mapping via Code Connect, iterative workflow without new screenshots. 

  - id: 08-016
    difficulty: senior
    category: "Unknown"
    question: |
      Which Figma MCP tool retrieves design tokens (colors, spacing, typography)?
    options:
      - a) get_design_context
      - b) get_variable_defs [CORRECT]
      - c) get_metadata
      - d) get_screenshot
    correct: b
    explanation: |
      `get_variable_defs` retrieves design tokens like colors (--color-primary: #3B82F6), spacing (--spacing-md: 16px), and typography. Recommended workflow: get_metadata → get_design_context → get_variable_defs (once per project) → get_screenshot (only when visual reference needed). 

  - id: 08-017
    difficulty: intermediate
    category: "Unknown"
    question: |
      What command adds the Figma MCP server for remote access?
    options:
      - a) claude mcp install figma
      - b) claude mcp add --transport http figma https://mcp.figma.com/mcp [CORRECT]
      - c) claude figma connect
      - d) npm install @figma/mcp
    correct: b
    explanation: |
      For remote MCP (all Figma plans, any machine): `claude mcp add --transport http figma https://mcp.figma.com/mcp`. For desktop MCP (requires Figma desktop app with Dev Mode): `claude mcp add --transport http figma-desktop http://127.0.0.1:3845/mcp`. Official Figma MCP was announced in 2025. 

  - id: 08-018
    difficulty: senior
    category: "Unknown"
    question: |
      What are the 2 core primitives of MCP Apps architecture?
    options:
      - a) Functions and Routes
      - b) Commands and Events
      - c) Tools with UI metadata and UI resources (ui:// scheme) [CORRECT]
      - d) Plugins and Themes
    correct: c
    explanation: |
      MCP Apps (SEP-1865) introduce 2 core primitives:  1. **Tools with UI metadata** - Standard MCP tools annotated with rendering hints (input forms, output displays) 2. **UI resources (ui:// scheme)** - Resources that return UI components instead of data  Together, these allow MCP servers to provide both functionality and user interface, enabling richer agent-to-user interactions. 

  - id: 08-019
    difficulty: senior
    category: "Unknown"
    question: |
      What does the 'auto:N' threshold control in MCP tool configuration?
    options:
      - a) Maximum concurrent MCP connections
      - b) Cache size for tool results
      - c) Lazy loading - tools loaded only when description matches user intent, N = max tools to auto-load [CORRECT]
      - d) Retry count for failed tool calls
    correct: c
    explanation: |
      The `auto:N` threshold enables lazy loading for MCP tools. Instead of loading all tools at session start (which wastes context), tools are only loaded when their description matches the user's intent. N controls the maximum number of tools to auto-load per intent match. This reduces context usage for MCP servers with many tools. 

  - id: 08-020
    difficulty: intermediate
    category: "Unknown"
    question: |
      Which MCP server scored highest (9.0/10) in the production ecosystem evaluation?
    options:
      - a) Playwright MCP (8.8/10)
      - b) Context7 MCP (8.5/10)
      - c) Semgrep MCP - SAST, secrets, and supply chain analysis [CORRECT]
      - d) Kubernetes MCP (8.4/10)
    correct: c
    explanation: |
      Semgrep MCP scored 9.0/10 in the production MCP ecosystem evaluation. It provides static application security testing (SAST), secrets detection, and supply chain analysis. Top scores: Semgrep (9.0), Playwright (8.8), Context7 (8.5), Kubernetes (8.4). Evaluation criteria: reliability, token efficiency, production readiness. 

  - id: 09-001
    difficulty: senior
    category: "Unknown"
    question: |
      What are the three components of 'The Trinity' pattern?
    options:
      - a) Git, VSCode, Terminal
      - b) Plan Mode, Extended Thinking, Sequential Thinking [CORRECT]
      - c) Serena, Context7, Playwright
      - d) Agent, Skill, Command
    correct: b
    explanation: |
      The Trinity is the most powerful Claude Code pattern combining:  1. **Plan Mode** - Safe exploration without changes 2. **Extended Thinking** - Deep analysis (enabled by default in Opus 4.5) 3. **Sequential Thinking (MCP)** - Structured multi-step reasoning  Combined, these provide maximum understanding before taking action. Use for architecture decisions, complex debugging, and legacy modernization. 

  - id: 09-002
    difficulty: power
    category: "Unknown"
    question: |
      How do you toggle thinking mode in Claude Code (Opus 4.5+)?
    options:
      - a) Use 'ultrathink' keyword in your prompt
      - b) Alt+T (or Option+T on macOS) [CORRECT]
      - c) --think CLI flag
      - d) /thinking command
    correct: b
    explanation: |
      With Opus 4.5 (v2.0.67+), thinking mode is enabled by default at maximum budget.  **Controlling Thinking Mode:** | Method | Action | Persistence | |--------|--------|-------------| | **Alt+T** | Toggle on/off | Session | | **/config** | Enable/disable globally | Permanent |  **Note**: Keywords like "ultrathink" are now cosmetic only - they no longer control thinking behavior.  Use Alt+T to disable thinking for simple tasks (faster, cheaper). 

  - id: 09-003
    difficulty: senior
    category: "Unknown"
    question: |
      What CLI flag runs Claude Code without interactive prompts for CI/CD?
    options:
      - a) --ci
      - b) --headless [CORRECT]
      - c) --batch
      - d) --automated
    correct: b
    explanation: |
      The `--headless` flag runs Claude Code without interactive prompts:  ```bash # Basic headless execution claude --headless "Run the tests and report results"  # With timeout claude --headless --timeout 300 "Build the project" ```  Essential for CI/CD integration, automated pipelines, and scripted workflows. 

  - id: 09-004
    difficulty: junior
    category: "Unknown"
    question: |
      How do you pipe content to Claude Code with a prompt?
    options:
      - a) claude | cat file.txt -p 'analyze'
      - b) cat file.txt | claude -p 'analyze this code' [CORRECT]
      - c) claude < file.txt --prompt 'analyze'
      - d) file.txt > claude -p 'analyze'
    correct: b
    explanation: |
      Use standard Unix piping with the `-p` flag:  ```bash cat file.txt | claude -p 'analyze this code' git diff | claude -p 'explain these changes' npm test 2>&1 | claude -p 'summarize test failures' ```  This enables powerful shell integration for automated code analysis. 

  - id: 09-005
    difficulty: power
    category: "Unknown"
    question: |
      What is the 'Rev the Engine' pattern?
    options:
      - a) Running tests in parallel
      - b) Multiple rounds of write-critique-improve cycles [CORRECT]
      - c) Restarting Claude Code between tasks
      - d) Using higher compute models
    correct: b
    explanation: |
      The "Rev the Engine" pattern uses multiple rounds of critique for quality:  ``` Round 1: [Initial implementation] Critique: [What's wrong] Improvement: [Better version]  Round 2: [Improved implementation] Critique: [What's still wrong] Improvement: [Even better version]  Round 3: [Final implementation] Final check: [Verification] ```  Typically 3 rounds are recommended for quality work. 

  - id: 09-006
    difficulty: senior
    category: "Unknown"
    question: |
      What output format flag gets structured JSON from Claude for scripting?
    options:
      - a) --format json
      - b) --output-format json [CORRECT]
      - c) --json
      - d) --structured
    correct: b
    explanation: |
      Use `--output-format` to control response format:  | Format | Use Case | |--------|----------| | `text` | Human-readable output (default) | | `json` | Machine-parseable structured data | | `stream-json` | Real-time streaming for large outputs |  Example: ```bash git log --oneline -10 | claude -p 'Categorize commits' --output-format json ``` 

  - id: 09-007
    difficulty: junior
    category: "Unknown"
    question: |
      What is 'Vibe Coding' according to the guide?
    options:
      - a) Coding while listening to music
      - b) Rapid prototyping through natural conversation before committing to implementation [CORRECT]
      - c) Using AI to generate random code
      - d) Coding in a relaxed environment
    correct: b
    explanation: |
      Vibe Coding is rapid prototyping through natural conversation.  When to vibe code: - Early exploration: Testing if an approach works - Proof of concept: Quick validation before full implementation - Learning: Understanding a new library or pattern  Rules: - No production code - exploration only - Throw away freely - Focus on learning - Signal clearly: "This is vibe code, not for production" 

  - id: 09-008
    difficulty: power
    category: "Unknown"
    question: |
      What is the Verify Gate Pattern before creating a PR?
    options:
      - a) Just run tests
      - b) Build -> Lint -> Test -> Type-check -> THEN create PR [CORRECT]
      - c) Create PR first, then fix issues
      - d) Manual review only
    correct: b
    explanation: |
      The Verify Gate Pattern ensures all checks pass before PR creation:  ``` Build -> Lint -> Test -> Type-check -> THEN create PR ```  If ANY step fails: - Stop immediately - Report what failed and why - Suggest fixes - Do NOT proceed to PR creation  This prevents wasted CI cycles and review time. 

  - id: 09-009
    difficulty: senior
    category: "Unknown"
    question: |
      What is the key insight of 'Todo as Instruction Mirrors'?
    options:
      - a) Todos are just for tracking
      - b) What you write as a todo becomes Claude's instruction [CORRECT]
      - c) Todos should be vague for flexibility
      - d) Always use bullet points
    correct: b
    explanation: |
      The Mirror Principle: What you write as a todo becomes Claude's instruction.  Bad (vague todo -> vague execution): "Fix the bug"  Good (specific todo -> precise execution): "Fix null pointer in getUserById when user not found - return null instead of throwing"  Well-crafted todos guide Claude's execution with precision. 

  - id: 09-010
    difficulty: power
    category: "Unknown"
    question: |
      According to 'Continuous Improvement Mindset', what should you ask after every manual intervention?
    options:
      - a) Was this my fault?
      - b) How can I improve the process so this error can be avoided next time? [CORRECT]
      - c) Who is responsible for this?
      - d) Should I use a different AI?
    correct: b
    explanation: |
      After every manual intervention, ask: "How can I improve the process so this error or manual fix can be avoided next time?"  The improvement pipeline: 1. Can a linting rule catch it? -> Add lint rule 2. Can it go in conventions/docs? -> Add to CLAUDE.md or ADRs 3. Neither? -> Accept as edge case  The meta-skill: instead of fixing code, fix the system that produces the code. 

  - id: 09-011
    difficulty: junior
    category: "Unknown"
    question: |
      What is a Skeleton Project?
    options:
      - a) A project with no code
      - b) A minimal, working template that establishes patterns before full implementation [CORRECT]
      - c) A deprecated project
      - d) A project outline document
    correct: b
    explanation: |
      A skeleton project is a minimal, working template that establishes patterns.  Skeleton principles: 1. **It must run**: `pnpm dev` works from day 1 2. **One complete vertical**: Full stack for one feature 3. **Patterns, not features**: Shows HOW, not WHAT 4. **Minimal dependencies**: Only what's needed  Progression: Skeleton (Day 1) -> MVP (Week 1) -> Full (Month 1) 

  - id: 09-012
    difficulty: senior
    category: "Unknown"
    question: |
      What is OpusPlan mode?
    options:
      - a) Using only Opus for everything
      - b) Opus for planning, Sonnet for execution [CORRECT]
      - c) A special debugging mode
      - d) Planning without AI
    correct: b
    explanation: |
      OpusPlan mode combines model strengths: - **Planning**: Opus for high-level thinking - **Execution**: Sonnet for implementation  This provides strategic thinking + cost-effective execution.  ```bash /model opusplan Shift+Tab x 2  # Enter Plan Mode (Opus) # Design architecture... Shift+Tab      # Exit Plan Mode (Sonnet) # Implement the plan... ``` 

  - id: 09-013
    difficulty: power
    category: "Unknown"
    question: |
      When should you NOT use --dangerously-skip-permissions?
    options:
      - a) In CI/CD pipelines
      - b) On production systems or sensitive codebases [CORRECT]
      - c) For automated testing
      - d) In Docker containers
    correct: b
    explanation: |
      Never use `--dangerously-skip-permissions` on production systems or sensitive codebases.  Safe usage: - CI/CD pipelines with isolated environments - Automated testing with limited scope - Development containers  Unsafe usage: - Production systems - Codebases with secrets - Environments with sensitive data  The flag bypasses all permission prompts, creating security risks. 

  - id: 09-014
    difficulty: senior
    category: "Unknown"
    question: |
      What prompt format does the guide recommend for effective requests?
    options:
      - a) Simple natural language
      - b) WHAT, WHERE, HOW, VERIFY [CORRECT]
      - c) Subject, Body, Footer
      - d) Title, Description, Acceptance Criteria
    correct: b
    explanation: |
      The recommended prompt format:  - **WHAT**: Concrete deliverable - **WHERE**: File paths/locations - **HOW**: Constraints/approach - **VERIFY**: Success criteria  Example: ``` WHAT: Add input validation to the login form WHERE: src/components/LoginForm.tsx, src/schemas/auth.ts HOW: Use Zod schema validation, display errors inline VERIFY: Empty email shows error, invalid format shows error ``` 

  - id: 09-015
    difficulty: junior
    category: "Unknown"
    question: |
      What is the recommended learning progression for Claude Code?
    options:
      - a) Learn everything at once
      - b) Start with advanced features
      - c) Week 1: Basic commands -> Week 2: CLAUDE.md -> Week 3: Agents -> Month 2+: MCP servers [CORRECT]
      - d) Skip to MCP servers immediately
    correct: c
    explanation: |
      The guide recommends progressive learning:  1. **Week 1**: Basic commands, context management 2. **Week 2**: CLAUDE.md, permissions 3. **Week 3**: Agents and commands 4. **Month 2+**: MCP servers, advanced patterns  Start with simple, low-risk tasks and build up complexity gradually. This avoids overwhelm and builds solid fundamentals. 

  - id: 09-016
    difficulty: power
    category: "Unknown"
    question: |
      What git workflow enables working on multiple features simultaneously with isolated contexts?
    options:
      - a) Git stash
      - b) Git worktrees [CORRECT]
      - c) Git branches only
      - d) Git submodules
    correct: b
    explanation: |
      Git worktrees create multiple working directories from the same repository.  Benefits: - Work on multiple features simultaneously - Each worktree has independent Claude Code context - No need for stash/switch operations - Parallel testing while developing  ```bash git worktree add ../myproject-hotfix hotfix git worktree add ../myproject-feature-a feature-a ``` 

  - id: 09-017
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is SDD (Spec-Driven Development) and its key claim?
    options:
      - a) Speed-based Development - fastest approach
      - b) Spec-Driven Development - one well-structured iteration equals 8 unstructured ones [CORRECT]
      - c) Standard Driven Development - follows industry standards
      - d) Sequential Driven Development - linear workflow
    correct: b
    explanation: |
      SDD (Spec-Driven Development) means specifications BEFORE code. The key claim: one well-structured iteration equals 8 unstructured ones. CLAUDE.md IS your spec file. Best for APIs and contracts. High Claude fit (⭐⭐⭐). 

  - id: 09-018
    difficulty: intermediate
    category: "Unknown"
    question: |
      What format does BDD (Behavior-Driven Development) use for test scenarios?
    options:
      - a) Arrange-Act-Assert
      - b) Given-When-Then (Gherkin) [CORRECT]
      - c) Setup-Execute-Verify
      - d) Input-Process-Output
    correct: b
    explanation: |
      BDD uses Given-When-Then format (Gherkin). BDD is beyond testing - it's a collaboration process: (1) Discovery with devs/business, (2) Formulation with examples, (3) Automation via Cucumber. Example: Given product with 0 stock → When customer attempts purchase → Then system refuses. 

  - id: 09-019
    difficulty: junior
    category: "Unknown"
    question: |
      What are the three steps of the TDD cycle?
    options:
      - a) Plan-Develop-Test
      - b) Red-Green-Refactor [CORRECT]
      - c) Write-Run-Fix
      - d) Design-Code-Review
    correct: b
    explanation: |
      The classic TDD cycle: (1) Red - write failing test, (2) Green - minimal code to pass, (3) Refactor - clean up while tests stay green. With Claude: be explicit "Write FAILING tests that don't exist yet." TDD is a core workflow (⭐⭐⭐). 

  - id: 09-020
    difficulty: senior
    category: "Unknown"
    question: |
      How many development methodologies are documented in the methodologies reference?
    options:
      - a) 5 methodologies
      - b) 10 methodologies
      - c) 15 methodologies [CORRECT]
      - d) 20 methodologies
    correct: c
    explanation: |
      15 methodologies organized in a 6-tier pyramid: Tier 1 (Strategic - BMAD), Tier 2 (Specification - SDD, Doc-Driven, Req-Driven, DDD), Tier 3 (Behavior - BDD, ATDD, CDD), Tier 4 (Feature - FDD, Context Engineering), Tier 5 (Implementation - TDD, Eval-Driven, Multi-Agent), Tier 6 (Optimization). 

  - id: 09-021
    difficulty: senior
    category: "Unknown"
    question: |
      What is 'Context Engineering' as a methodology?
    options:
      - a) Engineering team context
      - b) Treating context as first-class design element: progressive disclosure, memory management, dynamic refresh [CORRECT]
      - c) Building context menus
      - d) Managing environment variables
    correct: b
    explanation: |
      Context Engineering treats context as a design element. Key concepts: Progressive Disclosure (let agent discover incrementally), Memory Management (conversation vs persistent memory), Dynamic Refresh (rewrite TODO list before response). High Claude fit (⭐⭐⭐) for long sessions. 

  - id: 09-022
    difficulty: power
    category: "Unknown"
    question: |
      What are the 5 layers of 'Mechanic Stacking'?
    options:
      - a) Plan Mode, Extended Thinking, Rev the Engine, Split-Role, Permutation [CORRECT]
      - b) Plan Mode, Sequential Thinking, Context7, Serena, Playwright
      - c) CLAUDE.md, Plan Mode, Extended Thinking, MCP Servers, Hooks
      - d) Plan Mode, Extended Thinking, Rev the Engine, Multi-Agent, Hooks
    correct: a
    explanation: |
      Mechanic Stacking layers 5 techniques for maximum Claude Code power:  1. **Plan Mode** - Safe exploration without changes 2. **Extended Thinking** - Deep internal reasoning 3. **Rev the Engine** - Progressive warm-up prompts 4. **Split-Role** - Separate architect/implementer/reviewer 5. **Permutation** - Systematic variation testing  Each layer compounds the previous one's effectiveness. 

  - id: 09-023
    difficulty: power
    category: "Unknown"
    question: |
      What is a 'Permutation Framework' in Claude Code?
    options:
      - a) A/B testing of UI variants
      - b) CLAUDE.md-driven systematic variation testing: define dimensions, generate variants, implement, evaluate [CORRECT]
      - c) Random code generation for benchmarks
      - d) Automated regression test suite
    correct: b
    explanation: |
      A Permutation Framework uses CLAUDE.md to drive systematic variation testing. The workflow: define variation dimensions (e.g., algorithm choice, data structure, caching strategy), generate all combinations, implement each variant, evaluate with metrics. This ensures you explore the solution space methodically rather than picking the first approach. 

  - id: 09-024
    difficulty: senior
    category: "Unknown"
    question: |
      What mental model describes the developer as an orchestrator of Claude instances?
    options:
      - a) Pipeline Manager
      - b) Agent Supervisor
      - c) You Are the Main Thread - CPU scheduler analogy where developer dispatches tasks to agents [CORRECT]
      - d) Task Router
    correct: c
    explanation: |
      "You Are the Main Thread" uses the CPU scheduler analogy: the developer is the main thread dispatching work to Claude instances (worker threads). You manage priorities, context switches, and synchronization. Key insight: you don't write code - you manage the agents that write code. This shifts the skill from coding to orchestration. 

  - id: 09-025
    difficulty: senior
    category: "Unknown"
    question: |
      When task list items consistently diverge from actual work done, what does the guide recommend?
    options:
      - a) Delete the task list and start over
      - b) Add more granular detail to each task
      - c) Use divergence patterns as diagnostic: too broad = break down, too narrow = step back, wrong priorities = re-align with goals [CORRECT]
      - d) Ignore the divergence and continue
    correct: c
    explanation: |
      Task list divergence is diagnostic, not failure. Patterns reveal issues:  - **Too broad** tasks → break down into smaller pieces - **Too narrow** tasks → step back and think bigger - **Wrong priorities** → re-align tasks with actual goals - **Consistent drift** → your mental model of the problem is wrong  Use divergence as signal, not noise. 

  - id: 09-026
    difficulty: senior
    category: "Unknown"
    question: |
      What is the 'occurrence rule' for claiming established patterns in code reviews?
    options:
      - a) >5 occurrences = established, 2-5 = emerging, <2 = not established
      - b) >10 occurrences = established, 3-10 = emerging, <3 = not established [CORRECT]
      - c) Based on file count, not occurrences
      - d) >20 occurrences = established, 5-20 = emerging, <5 = not established
    correct: b
    explanation: |
      Anti-hallucination safeguards for code reviews define the occurrence rule:  - **>10 occurrences** = established pattern (safe to reference) - **3-10 occurrences** = emerging pattern (mention cautiously) - **<3 occurrences** = NOT an established pattern (don't claim it is)  This prevents Claude from hallucinating "established conventions" from a few examples. 

  - id: 09-027
    difficulty: power
    category: "Unknown"
    question: |
      What are the 3 specialized agents in multi-agent code review?
    options:
      - a) Security Reviewer, Performance Analyst, UX Reviewer
      - b) Linter, Type Checker, Test Runner
      - c) Consistency Auditor, SOLID Analyst, Defensive Code Auditor [CORRECT]
      - d) Junior Reviewer, Senior Reviewer, Architect Reviewer
    correct: c
    explanation: |
      Multi-agent PR review uses 3 specialized agents:  1. **Consistency Auditor** - Checks naming conventions, import patterns, code style adherence 2. **SOLID Analyst** - Reviews architectural principles, dependency injection, single responsibility 3. **Defensive Code Auditor** - Validates error handling, input validation, edge cases  Each agent has anti-hallucination safeguards (occurrence rule, file-scoped claims). 

  - id: 09-028
    difficulty: senior
    category: "Unknown"
    question: |
      What is 'comprehension debt' according to Addy Osmani's '80% Problem'?
    options:
      - a) Documentation debt - missing docs
      - b) Code you shipped but don't fully understand - distinct from technical debt [CORRECT]
      - c) Review debt - unreviewed PRs
      - d) Design debt - skipped design phase
    correct: b
    explanation: |
      Comprehension debt is code you shipped but don't fully understand. It's distinct from technical debt (code you understand but know is suboptimal). With AI-generated code, comprehension debt grows silently: you accept suggestions without deep understanding. The fix: always be able to explain WHY, not just WHAT. 

  - id: 09-029
    difficulty: senior
    category: "Unknown"
    question: |
      What is the '4-step cycle' for CLAUDE.md as compounding memory (Boris Cherny)?
    options:
      - a) Write, Test, Deploy, Monitor
      - b) Plan, Execute, Review, Commit
      - c) Error, Rule, Read, Never repeated [CORRECT]
      - d) Ask, Implement, Validate, Ship
    correct: c
    explanation: |
      Boris Cherny's mental model for CLAUDE.md as compounding memory:  1. **Error** - Claude makes a mistake 2. **Rule** - You add a rule to CLAUDE.md preventing it 3. **Read** - Claude reads CLAUDE.md on every session start 4. **Never repeated** - The mistake never happens again  Goal: never correct Claude twice for the same mistake. CLAUDE.md compounds over time. 

  - id: 10-001
    difficulty: junior
    category: "Unknown"
    question: |
      What keyboard shortcut enters Plan Mode (toggles plan/execute)?
    options:
      - a) Ctrl+P
      - b) Shift+Tab [CORRECT]
      - c) Alt+P
      - d) Ctrl+Shift+P
    correct: b
    explanation: |
      `Shift+Tab` toggles between Plan Mode and Execute Mode.  Plan Mode navigation: - `Shift+Tab` once: Toggle plan/execute - `Shift+Tab` twice: Enter deep Plan Mode (with Opus in OpusPlan)  Plan Mode allows safe, read-only exploration before making changes. 

  - id: 10-002
    difficulty: junior
    category: "Unknown"
    question: |
      What keyboard shortcut rewinds to a previous checkpoint (undo Claude's changes)?
    options:
      - a) Ctrl+Z
      - b) Esc (double-tap) [CORRECT]
      - c) Ctrl+R
      - d) Alt+Z
    correct: b
    explanation: |
      Double-tap `Esc` (Esc×2) rewinds to the previous checkpoint.  This is equivalent to the `/rewind` command. It undoes Claude's recent changes in the current session without creating git commits.  Use when Claude made a mistake and you want to try a different approach. 

  - id: 10-003
    difficulty: junior
    category: "Unknown"
    question: |
      What does `/compact` do?
    options:
      - a) Compresses files on disk
      - b) Summarizes and compresses the conversation context [CORRECT]
      - c) Minimizes the terminal window
      - d) Reduces Claude's response length
    correct: b
    explanation: |
      `/compact` summarizes and compresses the conversation context.  Use `/compact` when: - Context usage reaches 70-90% - Responses become slow - Claude starts forgetting earlier context  This frees up context space while preserving important information. Check context usage with `/status`. 

  - id: 10-004
    difficulty: senior
    category: "Unknown"
    question: |
      At what context percentage should you run /compact according to best practices?
    options:
      - a) 50%
      - b) 70-90% [CORRECT]
      - c) 95%+
      - d) Only when errors occur
    correct: b
    explanation: |
      Context management guidelines:  | Context Level | Action | |--------------|--------| | 0-50% | Work freely | | 50-75% | Be selective | | **75-90%** | **Use `/compact`** | | 90%+ | Use `/clear` |  Proactive compaction at 70% prevents context degradation and maintains performance. 

  - id: 10-006
    difficulty: senior
    category: "Unknown"
    question: |
      What is the correct permission pattern to allow ALL git commands?
    options:
      - a) Bash(git)
      - b) Bash(git *) [CORRECT]
      - c) git:*
      - d) Bash(*git*)
    correct: b
    explanation: |
      The pattern `Bash(git *)` allows any git command.  Permission pattern examples: - `Bash(git *)` - Any git command - `Bash(npm test)` - Exactly "npm test" - `Edit` - All file edits - `mcp__serena__*` - All Serena tools  Wildcards (*) enable flexible permission matching. 

  - id: 10-007
    difficulty: junior
    category: "Unknown"
    question: |
      Where should project-specific CLAUDE.md be placed (committed to git)?
    options:
      - a) ~/.claude/CLAUDE.md
      - b) /project/.claude/CLAUDE.md
      - c) /project/CLAUDE.md [CORRECT]
      - d) ~/.config/claude/project.md
    correct: c
    explanation: |
      CLAUDE.md locations:  | Location | Scope | Committed | |----------|-------|-----------| | `~/.claude/CLAUDE.md` | All projects | N/A (global) | | `/project/CLAUDE.md` | This project | **Yes** | | `/project/.claude/CLAUDE.md` | Personal | No |  The root `CLAUDE.md` is committed and shared with the team. The `.claude/CLAUDE.md` is personal and should be in `.gitignore`. 

  - id: 10-008
    difficulty: senior
    category: "Unknown"
    question: |
      What does the --mcp-debug flag do?
    options:
      - a) Disables MCP servers
      - b) Debugs MCP server connections with verbose output [CORRECT]
      - c) Tests MCP configurations
      - d) Enables MCP auto-discovery
    correct: b
    explanation: |
      The `--mcp-debug` flag enables debug mode for MCP server connections.  MCP debugging techniques: ```bash claude --mcp-debug  # Debug all MCP connections /mcp               # View MCP status inside Claude Code ```  Use when MCP servers aren't connecting or tools aren't appearing. 

  - id: 10-009
    difficulty: power
    category: "Unknown"
    question: |
      If an MCP server name validation fails with pattern error, what characters ARE allowed?
    options:
      - a) Letters only
      - b) Letters, numbers, underscores, hyphens (max 64 chars) [CORRECT]
      - c) Any characters except spaces
      - d) Alphanumeric and periods
    correct: b
    explanation: |
      MCP server names must match: `^[a-zA-Z0-9_-]{1,64}`  Allowed: - Letters (a-z, A-Z) - Numbers (0-9) - Underscores (_) - Hyphens (-) - Maximum 64 characters  Not allowed: - Spaces - Special characters (@, #, etc.) - More than 64 characters 

  - id: 10-010
    difficulty: junior
    category: "Unknown"
    question: |
      What command shows session info including context usage and cost?
    options:
      - a) /info
      - b) /status [CORRECT]
      - c) /stats
      - d) /session
    correct: b
    explanation: |
      The `/status` command shows session information.  Output includes: - Model being used - Context usage percentage - Session cost - Token counts  Example output: `Model: Sonnet | Ctx: 45.2k | Cost: $1.23 | Ctx(u): 42.0%`  Use `/stats` for usage statistics with activity graphs. 

  - id: 10-011
    difficulty: senior
    category: "Unknown"
    question: |
      What flag limits maximum API spend in headless mode?
    options:
      - a) --cost-limit
      - b) --max-budget-usd [CORRECT]
      - c) --spending-cap
      - d) --budget
    correct: b
    explanation: |
      The `--max-budget-usd` flag sets maximum API spend (only with `--print`):  ```bash claude -p "analyze" --max-budget-usd 5.00 ```  This prevents runaway costs in automated pipelines. The operation stops if the budget is exceeded. 

  - id: 10-012
    difficulty: junior
    category: "Unknown"
    question: |
      How do you run a shell command directly from Claude Code prompt?
    options:
      - a) shell: command
      - b) !command [CORRECT]
      - c) $command
      - d) run command
    correct: b
    explanation: |
      The `!command` prefix runs a shell command directly.  Quick actions: - `!command` - Run shell command - `@filename` - Reference file - `Ctrl+C` - Cancel operation - `Ctrl+R` - Retry last  Example: `!git status` runs git status without Claude interpreting it. 

  - id: 10-013
    difficulty: senior
    category: "Unknown"
    question: |
      What is the correct way to resume a specific session by ID?
    options:
      - a) claude --session abc123
      - b) claude -r abc123 [CORRECT]
      - c) claude --load abc123
      - d) claude -s abc123
    correct: b
    explanation: |
      Use `-r` or `--resume` to resume a specific session:  ```bash claude -r abc123           # Resume session abc123 claude --resume abc123     # Same as above claude -c                  # Resume last session (short) claude --continue          # Resume last session (long) ```  Combine with `-p` for scripting: `claude -r abc123 -p "check status"` 

  - id: 10-014
    difficulty: power
    category: "Unknown"
    question: |
      What is the recommended .gitignore pattern for Claude Code files?
    options:
      - a) Ignore all .claude/ contents
      - b) Ignore settings.local.json and CLAUDE.local.md, keep agents/commands/hooks [CORRECT]
      - c) Don't ignore anything
      - d) Ignore only agents/
    correct: b
    explanation: |
      Recommended .gitignore:  ```gitignore # Claude Code - Personal (ignore) .claude/settings.local.json CLAUDE.local.md .claude/.serena/  # Claude Code - Team (keep/commit) # .claude/CLAUDE.md (project memory) # .claude/agents/ # .claude/commands/ # .claude/hooks/ # .claude/settings.json ```  This keeps team workflows shared while personal settings stay private.

  - id: 10-015
    difficulty: junior
    category: "Unknown"
    question: |
      What is the daily workflow morning setup according to the guide?
    options:
      - a) Just start coding
      - b) Git pull, /status, load project memory, review yesterday's progress [CORRECT]
      - c) Run all tests first
      - d) Clear all context
    correct: b
    explanation: |
      Morning setup workflow:  1. Git pull latest changes 2. Review context with `/status` 3. Load project memory (`/sc:load` if using Serena) 4. Review yesterday's progress  This ensures you start with fresh code and full context awareness. 

  - id: 10-016
    difficulty: senior
    category: "Unknown"
    question: |
      What flag allows Claude's tools to access directories outside the current working directory?
    options:
      - a) --include-dir
      - b) --add-dir [CORRECT]
      - c) --context-dir
      - d) --load-dir
    correct: b
    explanation: |
      Use `--add-dir` to allow tool access to additional directories:  ```bash claude --add-dir ../shared ../utils claude --add-dir packages/api ```  By default, Claude can only access files in the current working directory. Use --add-dir to extend permissions to other directories (e.g., shared libraries in a monorepo). 

  - id: 10-017
    difficulty: power
    category: "Unknown"
    question: |
      When Sequential Thinking MCP seems slow or unresponsive, what should you expect?
    options:
      - a) It's broken and needs restart
      - b) 10-30 second responses are normal due to significant compute [CORRECT]
      - c) Switch to a different MCP
      - d) Reduce the query complexity
    correct: b
    explanation: |
      Sequential Thinking uses significant compute - expect 10-30 second responses.  This is not an error, just be patient.  Tips for Sequential: - Works best with specific, well-defined problems - Good: "Debug why user authentication fails on mobile" - Bad: "Make the app better"  The longer response time reflects deeper analysis. 

  - id: 10-018
    difficulty: junior
    category: "Unknown"
    question: |
      What shortcut opens an external editor for composing long text input?
    options:
      - a) Ctrl+E
      - b) Ctrl+G [CORRECT]
      - c) Ctrl+O
      - d) Ctrl+L
    correct: b
    explanation: |
      `Ctrl+G` opens an external editor for composing long text.  Useful input shortcuts: - `Ctrl+A`: Jump to beginning of line - `Ctrl+E`: Jump to end of line - `Ctrl+W`: Delete previous word - `Ctrl+G`: Open external editor - `Tab`: Autocomplete file paths  The external editor allows comfortable editing of complex prompts. 

  - id: 10-019
    difficulty: junior
    category: "Unknown"
    question: |
      What does the '--from-pr' flag do in Claude Code v2.1.27?
    options:
      - a) Creates a new pull request
      - b) Resumes sessions linked to a GitHub PR number or URL [CORRECT]
      - c) Fetches all PR comments into context
      - d) Closes a pull request
    correct: b
    explanation: |
      The `--from-pr` flag (v2.1.27) lets you resume Claude Code sessions linked to a specific GitHub PR. Usage: `claude --from-pr 123` or `claude --from-pr https://github.com/org/repo/pull/123`. This loads the PR context (diff, comments, checks) directly into the session. 

  - id: 10-020
    difficulty: senior
    category: "Unknown"
    question: |
      What breaking change was introduced in Claude Code v2.1.19 for hook/command arguments?
    options:
      - a) Arguments removed entirely
      - b) Migration from $ARGUMENTS.0 (dot syntax) to $ARGUMENTS[0] (bracket syntax) [CORRECT]
      - c) New $PARAMS variable replaced $ARGUMENTS
      - d) Arguments now require JSON format
    correct: b
    explanation: |
      Claude Code v2.1.19 changed argument access syntax from dot notation (`$ARGUMENTS.0`) to bracket notation (`$ARGUMENTS[0]`). This breaking change affects all custom commands and hooks that reference arguments. Update your scripts to use the new bracket syntax. 

  - id: 10-021
    difficulty: junior
    category: "Unknown"
    question: |
      According to the guide's 'Myths vs Reality' section, what is the truth about 'hidden Claude Code features'?
    options:
      - a) Secret flags exist for power users
      - b) Hidden features require an enterprise plan
      - c) There are no hidden features - Anthropic uses progressive rollout, not secret flags [CORRECT]
      - d) Beta flags are stored in .claude/config
    correct: c
    explanation: |
      The Myths vs Reality section debunks the "hidden features" myth. Anthropic uses progressive rollout (features gradually enabled for users) rather than secret flags or hidden configurations. All features are documented in release notes. There is no secret power-user mode. 

  - id: 11-001
    difficulty: junior
    category: "Unknown"
    question: |
      What are the three developer patterns when using AI, according to the Learning with AI guide?
    options:
      - a) Beginner, Intermediate, Expert
      - b) Dependent, Avoidant, Augmented [CORRECT]
      - c) Passive, Active, Proactive
      - d) Consumer, Producer, Creator
    correct: b
    explanation: |
      The three patterns are: Dependent (copy-paste without understanding, can't debug AI code), Avoidant (refuses AI on principle, slower than peers), and Augmented (uses AI critically, understands everything). The goal is to become Augmented - using AI for leverage while maintaining deep understanding. 

  - id: 11-002
    difficulty: junior
    category: "Unknown"
    question: |
      What does UVAL stand for in the AI learning protocol?
    options:
      - a) Use, Validate, Apply, Learn
      - b) Understand, Verify, Apply, Learn [CORRECT]
      - c) Understand, Validate, Analyze, Learn
      - d) Use, Verify, Analyze, Log
    correct: b
    explanation: |
      UVAL stands for: Understand First (the 15-minute rule), Verify (ensure you actually learned), Apply (transform knowledge into skill through modification), Learn (capture insights for long-term retention). This protocol ensures you're learning, not just copying. 

  - id: 11-003
    difficulty: junior
    category: "Unknown"
    question: |
      In the Quick Self-Check, if you score 0-2 'yes' answers, what does this indicate?
    options:
      - a) You're an augmented developer
      - b) You're on track with room for optimization
      - c) You're at dependency risk - outsourcing thinking [CORRECT]
      - d) You're an AI avoidant
    correct: c
    explanation: |
      A score of 0-2 yes answers indicates dependency risk - you're outsourcing your thinking to AI. The 5 questions test: can you explain AI-generated code, have you debugged without AI, do you know WHY solutions work, could you write the code yourself, do you know AI's limitations. Low scores mean you should read the "Breaking Dependency" section. 

  - id: 11-004
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the '15-Minute Rule' in the UVAL protocol's 'Understand First' step?
    options:
      - a) Limit AI usage to 15 minutes per day
      - b) A 4-step protocol: State problem, Brainstorm approaches, Identify gaps, THEN ask AI [CORRECT]
      - c) Wait 15 minutes after getting AI response before using it
      - d) Spend 15 minutes explaining AI code to a colleague
    correct: b
    explanation: |
      The 15-minute rule is a specific protocol: (1) State the problem in ONE sentence (2 min), (2) Brainstorm 3 possible approaches (5 min), (3) Identify your knowledge gaps (3 min), (4) THEN ask AI with a much better question (5 min). This forces you to think before asking, resulting in better questions and faster learning. 

  - id: 11-005
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the 'Explain It Back' technique in the UVAL protocol's Verify step?
    options:
      - a) Ask AI to explain its own code
      - b) Write documentation for the code
      - c) If you can't explain the code to a colleague, you haven't learned it [CORRECT]
      - d) Record yourself explaining and review later
    correct: c
    explanation: |
      The rule is simple: if you can't explain the code to a colleague, you don't understand it. This is the verification step - you must be able to articulate WHY the solution works, not just THAT it works. The guide recommends using a /explain-back slash command that asks YOU to explain AI-generated code. 

  - id: 11-006
    difficulty: intermediate
    category: "Unknown"
    question: |
      In the 30-Day Progression Plan, what is the AI usage ratio for Week 1?
    options:
      - a) 0-20% AI usage [CORRECT]
      - b) 40-50% AI usage
      - c) 60-70% AI usage
      - d) 70-80% AI usage
    correct: a
    explanation: |
      Week 1 focuses on foundations with 0-20% AI usage: Days 1-2 build feature WITHOUT AI (0%), Days 4-5 refactor with AI review only (20%), Day 6 debug without AI (0%). The goal is to build (or rebuild) core skills without heavy AI reliance. Success criteria: can explain every line you wrote. 

  - id: 11-007
    difficulty: intermediate
    category: "Unknown"
    question: |
      What AI usage ratio does the 30-Day Plan recommend for Week 4 (Augmented stage)?
    options:
      - a) 30-40%
      - b) 50-60%
      - c) 70% [CORRECT]
      - d) 90-100%
    correct: c
    explanation: |
      Week 4 (Augmented stage) recommends 70% AI usage with the UVAL protocol. The progression is: Week 1 (0-20%), Week 2 (30-40%), Week 3 (50-60%), Week 4 (70%). Success criteria for Week 4: you're fast AND you understand everything. The cap at 70% ensures you maintain skills. 

  - id: 11-008
    difficulty: junior
    category: "Unknown"
    question: |
      According to the guide, what is the metaphor for AI in development?
    options:
      - a) AI is your co-pilot
      - b) AI is your GPS [CORRECT]
      - c) AI is your assistant
      - d) AI is your teacher
    correct: b
    explanation: |
      AI is compared to GPS: great for getting somewhere fast, dangerous if you lose the ability to navigate without it, truly useful when you understand the map AND use the GPS. A developer who only copy-pastes AI output is like a driver who can't read a map - fine until the GPS fails or someone asks for directions. 

  - id: 11-009
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is a 'Red Flag' sign of AI dependency according to the checklist?
    options:
      - a) Using AI for complex algorithms
      - b) Can't start coding without AI [CORRECT]
      - c) Using AI for code reviews
      - d) Asking AI to explain concepts
    correct: b
    explanation: |
      "Can't start without AI" is a red flag indicating you've outsourced problem decomposition. Other red flags: don't understand AI's code, can't debug AI errors, anxiety without AI, rejected in interviews, always ask "how" never "why", every solution looks the same. Immediate action: code 30 min daily without AI. 

  - id: 11-010
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the 'Apply' step's core principle in the UVAL protocol?
    options:
      - a) Apply the code directly to production
      - b) Transform knowledge into skill through MODIFICATION, not copying [CORRECT]
      - c) Apply code reviews to all AI-generated code
      - d) Apply unit tests to verify correctness
    correct: b
    explanation: |
      The Apply step requires modification, not copy-paste. You must change at least one element (variable name, structure, edge case handling). Why? Modification forces understanding. Copying is passive; modifying requires you to engage with the code and understand how it works. 

  - id: 11-011
    difficulty: senior
    category: "Unknown"
    question: |
      What CLAUDE.md configuration enables 'Learning Mode' with Claude Code?
    options:
      - a) ## Learning Mode - Ask me questions before generating code [CORRECT]
      - b) ## Strict Mode - Never generate code directly
      - c) ## Teaching Mode - Always explain before showing
      - d) ## Quiz Mode - Test before implementing
    correct: a
    explanation: |
      Learning Mode in CLAUDE.md prompts Claude to ask questions before generating: "What approaches have I considered?", "What specifically am I stuck on?", "What do I expect the solution to look like?". This implements the UVAL protocol's Understand step directly in your workflow. 

  - id: 11-012
    difficulty: senior
    category: "Unknown"
    question: |
      What hook event is recommended for capturing daily learning?
    options:
      - a) PreToolUse - before each command
      - b) PostToolUse - after each edit
      - c) Stop - when session ends [CORRECT]
      - d) Notification - on alerts
    correct: c
    explanation: |
      The learning-capture.sh hook uses the Stop event (session end) to prompt: "What's ONE thing you learned today?" This logs to ~/claude-learnings.md automatically. It's lightweight (asks one question) so you'll actually use it, unlike verbose learning journals. 

  - id: 11-013
    difficulty: junior
    category: "Unknown"
    question: |
      What is the 'Dependent' pattern's main risk according to the guide?
    options:
      - a) Slower than peers
      - b) Unemployable [CORRECT]
      - c) Left behind
      - d) Overworked
    correct: b
    explanation: |
      The Dependent pattern's risk is becoming unemployable. Signs: copy-paste without understanding, can't debug AI code, anxiety without AI. In interviews: can't whiteboard basics, struggles with "why this approach?", asks to look up fundamentals. You ship code you can't explain - when it breaks, you're stuck. 

  - id: 11-014
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the Weekly Self-Audit question that detects dependency?
    options:
      - a) How many lines of code did I write?
      - b) Am I faster than last month? Am I smarter? [CORRECT]
      - c) How much did I use AI this week?
      - d) Did I meet all my deadlines?
    correct: b
    explanation: |
      The key question is: "Am I faster than last month? Am I smarter?" If you're faster but NOT smarter, you're building dependency. Other weekly questions: What did I learn that I didn't know before? Could I have done this without AI? Did I understand everything I shipped? 

  - id: 11-015
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the recommended approach for 'Avoidant' developers (Pattern 2)?
    options:
      - a) Continue avoiding AI to maintain purity
      - b) Start with AI review of YOUR code, not AI-generated code [CORRECT]
      - c) Immediately switch to 100% AI usage
      - d) Wait until AI tools are more mature
    correct: b
    explanation: |
      For Avoidant developers, the guide recommends gradual adoption: start with AI reviewing YOUR code (you stay in control), then move to AI explaining concepts you implement, then AI-assisted work. This respects your instinct for understanding while gaining AI benefits. Pure avoidance means being slower without being smarter. 

  - id: 11-016
    difficulty: senior
    category: "Unknown"
    question: |
      What are the 3 failure modes identified in Addy Osmani's '80% Problem' of agentic coding?
    options:
      - a) Context loss, token overflow, hallucination
      - b) Overengineering, assumption propagation, sycophantic agreement [CORRECT]
      - c) Slow responses, wrong language, missed tests
      - d) Memory leaks, race conditions, deadlocks
    correct: b
    explanation: |
      Addy Osmani's "80% Problem" identifies 3 failure modes:  1. **Overengineering** - AI adds unnecessary complexity (abstraction layers, premature optimization) 2. **Assumption propagation** - One wrong assumption cascades through the codebase 3. **Sycophantic agreement** - AI agrees with bad ideas instead of pushing back  The 80% refers to AI getting you 80% of the way, with the last 20% requiring deep human understanding. 

  - id: 11-017
    difficulty: intermediate
    category: "Unknown"
    question: |
      What are the 3 symptoms of vibe coding 'context overload'?
    options:
      - a) Memory leaks, slow startup, API rate limits
      - b) Big-bang context dumps, 5K+ line prompts, performance degradation [CORRECT]
      - c) Lost files, broken imports, missing tests
      - d) Excessive commits, merge conflicts, branch proliferation
    correct: b
    explanation: |
      Context overload symptoms in vibe coding:  1. **Big-bang context dumps** - Pasting entire codebases into prompts 2. **5K+ line prompts** - Exceeding effective context window usage 3. **Performance degradation** - Claude's quality drops as context grows  The fix: progressive disclosure (feed context incrementally), use CLAUDE.md for persistent context, and start fresh sessions between phases. 

  - id: 12-001
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the core architecture pattern of Claude Code?
    options:
      - a) A DAG orchestrator with task planning
      - b) A simple while(tool_call) loop with no classifier [CORRECT]
      - c) A RAG pipeline with embeddings
      - d) A multi-agent system with intent router
    correct: b
    explanation: |
      Claude Code runs a remarkably simple `while(tool_call)` loop. There is NO intent classifier, task router, RAG/embedding pipeline, DAG orchestrator, or planner/executor split. The model itself decides when to call tools, which tools to call, and when it's done. This is the "agentic loop" pattern. 

  - id: 12-002
    difficulty: junior
    category: "Unknown"
    question: |
      How many core tools does Claude Code have?
    options:
      - a) 5 tools
      - b) 8 tools [CORRECT]
      - c) 12 tools
      - d) Unlimited (model-dependent)
    correct: b
    explanation: |
      Claude Code has exactly 8 core tools: Bash (universal adapter), Read (file contents), Edit (modify files), Write (create/overwrite), Grep (search contents), Glob (find files), Task (sub-agents), and TodoWrite (progress tracking). That's the entire arsenal. 

  - id: 12-003
    difficulty: intermediate
    category: "Unknown"
    question: |
      Which tool is described as Claude's 'swiss-army knife' and 'universal adapter'?
    options:
      - a) Read
      - b) Grep
      - c) Bash [CORRECT]
      - d) Task
    correct: c
    explanation: |
      Bash is Claude's swiss-army knife. It can run any CLI tool (git, npm, docker, curl...), execute scripts, chain commands with pipes, and access system state. The model has been trained on massive amounts of shell data, making it highly effective as a universal adapter. 

  - id: 12-004
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the approximate context budget for Claude Code with Claude 3.5 Sonnet?
    options:
      - a) ~32K tokens
      - b) ~100K tokens
      - c) ~200K tokens [CORRECT]
      - d) ~500K tokens
    correct: c
    explanation: |
      Claude Code operates within a ~200K token context window. This is shared between: system prompt (~5-15K), CLAUDE.md files (~1-10K), conversation history (variable), tool results (variable), and reserved response buffer (~40-45K). Usable space is approximately 140-150K tokens. 

  - id: 12-005
    difficulty: senior
    category: "Unknown"
    question: |
      What are the reported auto-compaction thresholds for Claude Code?
    options:
      - a) 50-60%
      - b) 75-92% (conflicting reports) [CORRECT]
      - c) 95-99%
      - d) No auto-compaction exists
    correct: b
    explanation: |
      Auto-compaction thresholds vary by source: PromptLayer analysis reports 92%, community observations report 75-80%. When triggered, older conversation turns are summarized, tool results condensed, and recent context preserved. Use /compact to manually trigger summarization. 

  - id: 12-006
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the maximum depth for sub-agents spawned via the Task tool?
    options:
      - a) Unlimited depth
      - b) Depth = 3
      - c) Depth = 1 (cannot spawn sub-sub-agents) [CORRECT]
      - d) Depth = 2
    correct: c
    explanation: |
      Sub-agents have a depth=1 limit. They CANNOT spawn sub-sub-agents. This prevents: recursive explosion (infinite resources), context pollution (accumulated context), debugging nightmares (multi-level chains), and unpredictable costs (nested token usage). 

  - id: 12-007
    difficulty: senior
    category: "Unknown"
    question: |
      What does a sub-agent receive when spawned by the Task tool?
    options:
      - a) Full conversation history + all file reads
      - b) Task description only (isolated fresh context) [CORRECT]
      - c) Last 10 messages of conversation
      - d) System prompt + CLAUDE.md files
    correct: b
    explanation: |
      Sub-agents have ISOLATED context. They receive only the task description, have their own fresh context window, access the same tools (except Task), and return only a summary text. This isolation keeps the main context clean and prevents context pollution. 

  - id: 12-008
    difficulty: intermediate
    category: "Unknown"
    question: |
      What are the 4 permission layers in Claude Code's security model?
    options:
      - a) User, Admin, System, Root
      - b) Interactive prompts, Allow/Deny rules, Hooks, Sandbox [CORRECT]
      - c) Read, Write, Execute, Delete
      - d) Local, Project, Global, Enterprise
    correct: b
    explanation: |
      Claude Code has 4 layered security: (1) Interactive prompts (allow once/always/deny), (2) Allow/Deny rules in settings.json, (3) Hooks (Pre/Post execution scripts), (4) Sandbox mode (filesystem + network isolation). Each layer adds protection. 

  - id: 12-009
    difficulty: senior
    category: "Unknown"
    question: |
      What algorithm does the Edit tool use when exact match fails?
    options:
      - a) Returns error immediately
      - b) Fuzzy match (whitespace normalization, line ending normalization, context expansion) [CORRECT]
      - c) Regex pattern matching
      - d) Semantic similarity search
    correct: b
    explanation: |
      When exact match fails, Edit attempts fuzzy matching: (1) Whitespace normalization (trailing spaces, indentation), (2) Line ending normalization (CRLF vs LF), (3) Context expansion (surrounding lines). Only if fuzzy match also fails does it return an error. 

  - id: 12-010
    difficulty: intermediate
    category: "Unknown"
    question: |
      What protocol does MCP (Model Context Protocol) use for communication?
    options:
      - a) REST API over HTTPS
      - b) GraphQL
      - c) JSON-RPC 2.0 over stdio or HTTP [CORRECT]
      - d) gRPC with Protocol Buffers
    correct: c
    explanation: |
      MCP uses JSON-RPC 2.0 over stdio or HTTP transport. MCP tools follow the naming convention `mcp__<server>__<tool>`. Servers start on first use and stay alive during the session. They have the same permission system as native tools. 

  - id: 12-011
    difficulty: senior
    category: "Unknown"
    question: |
      What is Claude Code's design philosophy, as stated by Anthropic?
    options:
      - a) More scaffolding, less model - build complex orchestration
      - b) Less scaffolding, more model - trust Claude's reasoning [CORRECT]
      - c) Maximum control - explicit rules for every case
      - d) Hybrid approach - RAG + classifier + model
    correct: b
    explanation: |
      Claude Code's philosophy is "Less scaffolding, more model" - trust Claude's reasoning instead of building complex orchestration systems. This means: single model decides (no classifier/router), Grep+Glob (no RAG), simple while loop (no DAG), conversation as state (no state machines). 

  - id: 12-012
    difficulty: senior
    category: "Unknown"
    question: |
      What are the 4 specialized sub-agent types available in Claude Code?
    options:
      - a) Reader, Writer, Searcher, Executor
      - b) Explore, Plan, Bash, general-purpose [CORRECT]
      - c) Junior, Senior, Expert, Architect
      - d) Fast, Balanced, Thorough, Complete
    correct: b
    explanation: |
      Claude Code offers 4 sub-agent types: Explore (codebase exploration, read-only tools), Plan (architecture planning, no Edit/Write), Bash (command execution, Bash only), and general-purpose (complex multi-step tasks, all tools). Each has different tool access. 

  - id: 12-013
    difficulty: junior
    category: "Unknown"
    question: |
      What tool replaced TodoWrite for task management in Claude Code v2.1.16+?
    options:
      - a) ProjectManager API
      - b) Kanban tool
      - c) Tasks API (TaskCreate, TaskGet, TaskList, TaskUpdate) [CORRECT]
      - d) AgendaWrite tool
    correct: c
    explanation: |
      The Tasks API (v2.1.16+) replaced TodoWrite with 4 tools: TaskCreate (create tasks with descriptions), TaskGet (fetch full task details), TaskList (list all tasks with status), TaskUpdate (update status, dependencies, metadata). Key improvement: task dependencies via blockedBy/blocks fields. 

  - id: 12-014
    difficulty: power
    category: "Unknown"
    question: |
      What is the API cost overhead of fetching full details for 50 tasks via Tasks API?
    options:
      - a) 10x overhead (10 API calls)
      - b) 25x overhead (25 API calls)
      - c) 51x overhead (1 TaskList + 50 TaskGet calls required) [CORRECT]
      - d) 100x overhead (100 API calls)
    correct: c
    explanation: |
      Tasks API limitation: TaskList returns only summary fields (id, subject, status, owner, blockedBy). To get full details (description, metadata), you need individual TaskGet calls. For 50 tasks: 1 TaskList + 50 TaskGet = 51 API calls. This N+1 pattern is a known limitation compared to TodoWrite which returned everything in one call. 

  - id: 12-015
    difficulty: power
    category: "Unknown"
    question: |
      What is TeammateTool and what is its status?
    options:
      - a) Production-ready multi-agent framework
      - b) Experimental multi-agent coordination (spawnTeam, discoverTeams, requestJoin, approveJoin) - unstable, no official support [CORRECT]
      - c) Official Anthropic tool for team collaboration
      - d) Deprecated feature replaced by Tasks API
    correct: b
    explanation: |
      TeammateTool is an experimental multi-agent coordination tool with 4 operations: spawnTeam, discoverTeams, requestJoin, approveJoin. Status: unstable, not officially supported by Anthropic. It allows Claude instances to form teams and coordinate, but the API is subject to change without notice. Not recommended for production use. 

  - id: 13-001
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is an 'MCP Rug Pull' attack?
    options:
      - a) An MCP server that crashes unexpectedly
      - b) A benign MCP that turns malicious after gaining trust (no re-approval needed) [CORRECT]
      - c) An MCP that uses too many tokens
      - d) An attack on the MCP protocol itself
    correct: b
    explanation: |
      An MCP Rug Pull exploits the one-time approval model: attacker publishes benign MCP → user approves once → MCP works normally (builds trust) → attacker pushes malicious update → MCP exfiltrates credentials WITHOUT re-approval. Mitigation: version pinning + hash verification. 

  - id: 13-002
    difficulty: senior
    category: "Unknown"
    question: |
      What does CVE-2025-53109/53110 (EscapeRoute) exploit?
    options:
      - a) Prompt injection in Claude's system prompt
      - b) Filesystem MCP sandbox escape via prefix bypass + symlinks [CORRECT]
      - c) Memory corruption in the Bash tool
      - d) API key leakage in network requests
    correct: b
    explanation: |
      CVE-2025-53109/53110 (EscapeRoute) allows sandbox escape in Filesystem MCP via prefix bypass combined with symlinks. Severity: High. Mitigation: avoid Filesystem MCP with unrestricted access or apply the official patch. Source: Cymulate security research. 

  - id: 13-003
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is a known limitation of permissions.deny in .claude/settings.json?
    options:
      - a) It only works on macOS
      - b) System reminders may expose file contents before tool permission checks [CORRECT]
      - c) It cannot block Bash commands
      - d) It requires admin privileges
    correct: b
    explanation: |
      permissions.deny has architectural limitations: background indexing may expose file contents via internal "system reminder" mechanism BEFORE tool permission checks are applied. This is documented in GitHub #4160. Defense-in-depth: store secrets outside project directories. 

  - id: 13-004
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the recommended defense-in-depth strategy for secrets protection?
    options:
      - a) Only use permissions.deny
      - b) Store secrets outside project + external vault + PreToolUse hooks + never commit [CORRECT]
      - c) Encrypt all files in the project
      - d) Use a VPN when running Claude Code
    correct: b
    explanation: |
      Defense-in-depth: (1) Store secrets outside project directories (~/.secrets/ or vault), (2) Use external secrets management (AWS Secrets Manager, 1Password), (3) Add PreToolUse hooks as secondary blocking, (4) Never commit secrets, (5) Manually review bash commands. 

  - id: 13-005
    difficulty: senior
    category: "Unknown"
    question: |
      Which prompt injection evasion technique uses U+200B, U+200C, U+200D?
    options:
      - a) Base64 encoding
      - b) RTL override
      - c) Zero-width characters (invisible to humans) [CORRECT]
      - d) Homoglyphs
    correct: c
    explanation: |
      Zero-width characters (U+200B, U+200C, U+200D) make instructions invisible to humans while still being interpreted. Detection: Unicode regex pattern [\x{200B}-\x{200D}\x{FEFF}\x{202A}-\x{202E}]. Added to prompt-injection-detector.sh in v3.6.0. 

  - id: 13-006
    difficulty: intermediate
    category: "Unknown"
    question: |
      Which secret detection tool has the highest recall (88%) but lower precision (46%)?
    options:
      - a) TruffleHog
      - b) GitGuardian
      - c) Gitleaks [CORRECT]
      - d) detect-secrets
    correct: c
    explanation: |
      Gitleaks: 88% recall, 46% precision, fast (~2 min/100K commits) - best for pre-commit hooks. TruffleHog: 52% recall, 85% precision, slow - best for CI verification. GitGuardian: 80% recall, 95% precision - enterprise monitoring. detect-secrets: 60% recall, 98% precision - baseline approach. 

  - id: 13-007
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the recommended hook stack for security in settings.json?
    options:
      - a) Only PostToolUse hooks for logging
      - b) PreToolUse (dangerous blocker, injection detector) + PostToolUse (output scanner) + SessionStart (MCP integrity) [CORRECT]
      - c) No hooks - rely only on permissions.deny
      - d) Only UserPromptSubmit hooks
    correct: b
    explanation: |
      Recommended security hook stack: PreToolUse → dangerous-actions-blocker.sh (Bash), prompt-injection-detector.sh + unicode-injection-scanner.sh (Edit/Write). PostToolUse → output-secrets-scanner.sh (Bash). SessionStart → mcp-config-integrity.sh. Multiple layers for defense-in-depth. 

  - id: 13-008
    difficulty: junior
    category: "Unknown"
    question: |
      Which MCP servers are marked as 'Safe' in the community-vetted safe list?
    options:
      - a) filesystem (unrestricted), database (prod credentials)
      - b) @anthropic/mcp-server-*, context7, sequential-thinking, memory [CORRECT]
      - c) browser (full access), custom MCPs
      - d) All MCPs are safe by default
    correct: b
    explanation: |
      MCP Safe List: @anthropic/mcp-server-* (official), context7 (read-only docs), sequential-thinking (no external access, local), memory (local file-based). Risk: filesystem unrestricted (CVE-2025-53109), database prod (exfiltration). Unsafe: browser full access. 

  - id: 13-009
    difficulty: junior
    category: "Unknown"
    question: |
      What is the first action when a secret is exposed?
    options:
      - a) Document the incident for post-mortem
      - b) Revoke the credential immediately [CORRECT]
      - c) Scan the entire repo
      - d) Notify the team
    correct: b
    explanation: |
      First 15 minutes (stop the bleeding): (1) Revoke immediately - AWS delete-access-key, GitHub revoke token, Stripe roll key. (2) Confirm exposure scope. Then: audit git history, scan dependencies, check CI/CD logs. First 24 hours: rotate ALL related credentials, notify compliance, document timeline. 

  - id: 13-010
    difficulty: junior
    category: "Unknown"
    question: |
      What are the three security posture levels in the guide?
    options:
      - a) Low, Medium, High
      - b) Basic (5 min), Standard (30 min), Hardened (2 hours) [CORRECT]
      - c) Development, Staging, Production
      - d) Free, Pro, Enterprise
    correct: b
    explanation: |
      Security posture levels: Basic (5 min) = output scanner + dangerous blocker - for solo dev/experiments. Standard (30 min) = + injection hooks + MCP vetting - for teams/sensitive code. Hardened (2 hours) = + integrity verification + ZDR - for enterprise/production. 

  - id: 13-011
    difficulty: senior
    category: "Unknown"
    question: |
      Which sandbox isolation approach combines microVM isolation with network policies for agent autonomy?
    options:
      - a) E2B (hosted cloud sandboxes)
      - b) Fly.io Sprites (edge compute)
      - c) Docker Sandboxes (with custom templates and network policies) [CORRECT]
      - d) Cloudflare Sandbox SDK
    correct: c
    explanation: |
      Docker Sandboxes provide microVM-level isolation with customizable network policies. Key features: custom Dockerfile templates for reproducible environments, network policies to control egress/ingress, volume mounts for persistent storage, and CPU/memory limits. This approach suits teams wanting full control over sandbox configuration while maintaining strong isolation. 

  - id: 13-012
    difficulty: junior
    category: "Unknown"
    question: |
      What is the GitHub Issue Auto-Creation Bug (#13797) and why is it dangerous?
    options:
      - a) Issues get automatically deleted
      - b) Claude Code accidentally creates public GitHub issues containing private project details [CORRECT]
      - c) Issues are created but remain private
      - d) Only affects paid enterprise accounts
    correct: b
    explanation: |
      The GitHub Issue Auto-Creation Bug (#13797) causes Claude Code to accidentally create public GitHub issues containing private project details. Over 17 accidental disclosures documented, affecting v2.0.65+. The danger: internal code, architecture decisions, and private context leak publicly. Mitigation: disable the gh CLI tool or restrict GitHub permissions. 

  - id: 14-001
    difficulty: junior
    category: "Unknown"
    question: |
      What is the default data retention period for Claude Code conversations?
    options:
      - a) 30 days
      - b) 1 year
      - c) 5 years [CORRECT]
      - d) Forever
    correct: c
    explanation: |
      Default retention is 5 years with data used for model training. By opting out of training at claude.ai/settings/data-privacy-controls, retention reduces to 30 days (safety monitoring only). Enterprise API (ZDR) has 0-day retention. 

  - id: 14-002
    difficulty: intermediate
    category: "Unknown"
    question: |
      What data is sent to Anthropic when using Claude Code?
    options:
      - a) Only your prompts
      - b) Prompts, files Claude reads, MCP results, Bash outputs, error messages [CORRECT]
      - c) Only code snippets you copy-paste
      - d) Hashed metadata only
    correct: b
    explanation: |
      Everything Claude sees is sent: your prompts, files Claude reads (including .env if not excluded!), MCP server results (SQL queries, API responses), Bash command outputs, and error messages with stack traces. Use permissions.deny to block sensitive files. 

  - id: 14-003
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the risk when connecting a database via MCP?
    options:
      - a) Database might slow down
      - b) Query results (including PII) are sent to Anthropic and stored per retention policy [CORRECT]
      - c) Claude might drop tables
      - d) MCP uses too many tokens
    correct: b
    explanation: |
      When MCP executes a database query, ALL results are sent to Anthropic: "SELECT * FROM orders" → 100 rows with customer names, emails, addresses → stored according to your retention tier. NEVER connect production databases. Use dev/staging with anonymized data. 

  - id: 14-004
    difficulty: junior
    category: "Unknown"
    question: |
      How can you reduce Claude Code data retention from 5 years to 30 days?
    options:
      - a) Delete ~/.claude folder
      - b) Disable 'Allow model training' at claude.ai/settings/data-privacy-controls [CORRECT]
      - c) Use incognito mode
      - d) Add --no-retention flag
    correct: b
    explanation: |
      Visit claude.ai/settings/data-privacy-controls and toggle OFF "Allow model training". This immediately reduces retention from 5 years to 30 days (safety monitoring only). Enterprise API (ZDR) provides 0-day retention for HIPAA/GDPR compliance. 

  - id: 14-005
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the Enterprise API (ZDR) data retention policy?
    options:
      - a) 30 days retention
      - b) 1 year retention
      - c) 0 days (real-time processing only, data not stored) [CORRECT]
      - d) 5 years like default
    correct: c
    explanation: |
      Enterprise API (Zero Data Retention) has 0-day retention - data is processed in real-time and not stored. Required for HIPAA, GDPR, PCI-DSS compliance and government contracts. Requires enterprise contract with Anthropic. 

  - id: 14-006
    difficulty: junior
    category: "Unknown"
    question: |
      What is the recommended session search tool from the guide?
    options:
      - a) claude-conversation-extractor
      - b) session-search.sh (zero-dependency bash script) [CORRECT]
      - c) ran CLI (npm)
      - d) Built-in /search command
    correct: b
    explanation: |
      session-search.sh is the recommended tool: zero dependencies (bash only), fast (~10ms list, ~400ms search), displays ready-to-use 'claude --resume' commands. Install with alias 'cs' for quick access. Alternative Python tools exist but are slower. 

  - id: 14-007
    difficulty: intermediate
    category: "Unknown"
    question: |
      What hook event is used for session logging?
    options:
      - a) PreToolUse
      - b) PostToolUse [CORRECT]
      - c) SessionStart
      - d) Notification
    correct: b
    explanation: |
      Session logging uses PostToolUse hook - it runs after each tool completes, capturing tool name, file path, project, and token estimates. Configure in settings.json with the session-logger.sh script. Logs are stored as JSONL files in ~/.claude/logs/. 

  - id: 14-008
    difficulty: intermediate
    category: "Unknown"
    question: |
      What is the token estimation method used by the session logger?
    options:
      - a) API-provided exact counts
      - b) ~4 characters per token (heuristic, slightly overestimates) [CORRECT]
      - c) 1 word = 1 token
      - d) Based on file size only
    correct: b
    explanation: |
      The logger estimates tokens using ~4 characters per token heuristic. This is approximate and tends to slightly overestimate. Claude Code CLI doesn't expose actual API token metrics, so estimates have ~15-25% variance from actual billing. 

  - id: 14-009
    difficulty: intermediate
    category: "Unknown"
    question: |
      What CANNOT the session monitoring do?
    options:
      - a) Track tool usage counts
      - b) Identify file access patterns
      - c) Provide exact token counts and actual API costs [CORRECT]
      - d) Record operation timestamps
    correct: c
    explanation: |
      Monitoring CANNOT provide: exact token counts (CLI doesn't expose API metrics), actual API costs (estimates only), TTFT timing, real-time streaming metrics, or context window usage. It CAN track: tool usage counts, file access patterns, relative comparisons, operation timing. 

  - id: 14-010
    difficulty: junior
    category: "Unknown"
    question: |
      Where are Claude Code session logs stored locally?
    options:
      - a) ~/.config/claude/
      - b) ~/.claude/projects/<project>/ [CORRECT]
      - c) /var/log/claude/
      - d) In the cloud only
    correct: b
    explanation: |
      Sessions are stored locally at ~/.claude/projects/<project>/ as JSONL files. This enables session resume with 'claude --resume <id>' or 'claude -c' for most recent. Custom logs from session-logger.sh go to ~/.claude/logs/ (configurable via CLAUDE_LOG_DIR). 

  - id: 14-011
    difficulty: intermediate
    category: "Unknown"
    question: |
      When should you use 'Co-Authored-By' vs 'Assisted-By' in git commits?
    options:
      - a) Always use Co-Authored-By for any AI involvement
      - b) Co-Authored-By when AI generated significant code, Assisted-By when AI only advised or reviewed [CORRECT]
      - c) Never attribute AI in commits
      - d) Use both on every commit
    correct: b
    explanation: |
      AI traceability in git commits:  - **Co-Authored-By**: When AI generated significant portions of code (implementation, refactoring) - **Assisted-By**: When AI only advised, reviewed, or suggested minor changes  This distinction matters for code ownership, audit trails, and understanding who (or what) actually wrote the code. Some teams add this to CLAUDE.md as a mandatory commit convention. 

  - id: 15-001
    difficulty: junior
    category: "Unknown"
    question: |
      What is the main philosophy behind using complementary AI tools with Claude Code?
    options:
      - a) Replace Claude Code with better tools
      - b) Augmentation, not replacement - chain the right tool for each step [CORRECT]
      - c) Use as many tools as possible simultaneously
      - d) Avoid using Claude Code for coding tasks
    correct: b
    explanation: |
      The philosophy is "augmentation, not replacement."  Claude Code excels at contextual reasoning and implementation, but has gaps: - No real-time web search with source verification - No image generation - No PowerPoint/slide generation - No audio synthesis  The goal is to chain the right tool for each step of your workflow. 

  - id: 15-002
    difficulty: junior
    category: "Unknown"
    question: |
      When should you use Perplexity instead of Claude Code?
    options:
      - a) When implementing features in your codebase
      - b) When you need deep research with 100+ verified sources [CORRECT]
      - c) When editing multiple files
      - d) When running tests
    correct: b
    explanation: |
      Perplexity excels at research with verified sources.  Use Perplexity for: - "What's the latest API for X?" (real-time info) - "Compare 5 libraries for auth" (sourced comparisons) - Deep Research mode synthesizes 100+ sources  Use Claude Code for: - Implementation in your codebase - Contextual code explanation - Multi-file editing 

  - id: 15-003
    difficulty: senior
    category: "Unknown"
    question: |
      What is the recommended Research-to-Code pipeline?
    options:
      - a) Claude Code -> Perplexity -> Implementation
      - b) Perplexity Deep Research -> Export spec.md -> Claude Code implements [CORRECT]
      - c) Google Search -> Copy code -> Paste in project
      - d) Ask Claude Code to search the web first
    correct: b
    explanation: |
      The Research-to-Code pipeline:  1. PERPLEXITY (Deep Research)    - Research best practices with sources    - Output: 2000-word spec with citations  2. Export as spec.md  3. CLAUDE CODE    - "Implement following spec.md"    - Output: Working implementation with tests  This ensures implementation is based on verified, current information. 

  - id: 15-004
    difficulty: senior
    category: "Unknown"
    question: |
      Which tool is best for converting a UI screenshot to React code?
    options:
      - a) Perplexity
      - b) Claude Code directly
      - c) Gemini 2.5 Pro -> then Claude Code for refinement [CORRECT]
      - d) NotebookLM
    correct: c
    explanation: |
      Gemini 2.5 Pro excels at visual understanding:  Visual-to-Code Pipeline: 1. GEMINI: Upload screenshot -> Get JSX + Tailwind (90%+ fidelity) 2. CLAUDE CODE: Refine for your project    - Add TypeScript types    - Connect to your components    - Integrate with your state management  Claude Code alone has limited image understanding compared to Gemini. 

  - id: 15-005
    difficulty: junior
    category: "Unknown"
    question: |
      Which tool should you use to create PowerPoint presentations from code documentation?
    options:
      - a) Claude Code
      - b) Perplexity
      - c) Kimi (Moonshot AI) [CORRECT]
      - d) Gemini
    correct: c
    explanation: |
      Kimi (kimi.ai) from Moonshot AI specializes in PPTX generation:  Features: - Native PPTX export (actual slides, not markdown) - 128K+ token context (entire codebases) - Code-aware layouts with syntax highlighting - Excellent for stakeholder decks from technical content  Workflow: Claude Code generates summary -> Kimi creates presentation 

  - id: 15-006
    difficulty: senior
    category: "Unknown"
    question: |
      What is NotebookLM's unique feature for developer onboarding?
    options:
      - a) Code execution
      - b) Audio Overview - generates podcast-style explanations from docs [CORRECT]
      - c) Real-time collaboration
      - d) Git integration
    correct: b
    explanation: |
      NotebookLM's Audio Overview feature:  - Upload 50+ documentation files - Generates 10-15 minute "podcast" - Two AI hosts discuss your codebase - Perfect for onboarding or reviewing large systems  Workflow: 1. Export docs to combined-docs.md 2. Upload to NotebookLM 3. Generate Audio Overview 4. Listen during commute 5. Return to Claude Code with deeper understanding 

  - id: 15-007
    difficulty: power
    category: "Unknown"
    question: |
      In the complete workflow pipeline, what is the correct order of phases?
    options:
      - a) Implementation -> Planning -> Delivery
      - b) Planning (Perplexity/Gemini/NotebookLM) -> Implementation (Claude Code) -> Delivery (Kimi) [CORRECT]
      - c) Delivery -> Planning -> Implementation
      - d) All tools simultaneously
    correct: b
    explanation: |
      The complete pipeline has 3 phases:  PLANNING PHASE: - Perplexity: Deep Research -> spec.md - Gemini: Diagram Analysis -> mermaid + plan - NotebookLM: Doc Synthesis -> audio overview  IMPLEMENTATION PHASE: - Claude Code: Multi-file implementation - IDE + Copilot: Inline autocomplete  DELIVERY PHASE: - Claude Code: PR description, release notes - Kimi: Stakeholder deck (presentation.pptx) 

  - id: 15-008
    difficulty: junior
    category: "Unknown"
    question: |
      What is v0.dev best used for in the AI ecosystem?
    options:
      - a) Backend API development
      - b) Database queries
      - c) Rapid UI prototyping with Shadcn/Tailwind [CORRECT]
      - d) Security auditing
    correct: c
    explanation: |
      v0.dev excels at rapid UI prototyping:  - Generates React + Shadcn + Tailwind components - Live preview in browser - Quick iteration on UI designs  Workflow: 1. v0: Prompt -> Get component preview 2. Export code 3. Claude Code: Adapt for your project    - Add TypeScript types    - Connect to your APIs    - Integrate with your design system 

  - id: 15-009
    difficulty: senior
    category: "Unknown"
    question: |
      How does Claude WebSearch compare to Perplexity Pro for research?
    options:
      - a) Claude WebSearch has more sources (100+)
      - b) They are identical in functionality
      - c) Claude WebSearch: 5-10 sources, quick lookups. Perplexity Pro: 100+ sources, comprehensive research [CORRECT]
      - d) Perplexity cannot access real-time data
    correct: c
    explanation: |
      Comparison:  | Feature | Claude WebSearch | Perplexity Pro | |---------|-----------------|----------------| | Source count | ~5-10 | 100+ (Deep Research) | | Source verification | Basic | Full citations | | Best for | Quick lookups | Comprehensive research | | Cost | Included | $20/month |  Recommendation: Use Claude WebSearch for quick factual checks. Use Perplexity Deep Research before significant implementations. 

  - id: 15-010
    difficulty: power
    category: "Unknown"
    question: |
      What is the 'Minimal Stack' monthly cost recommendation for AI tools?
    options:
      - a) $200+/month - all Pro subscriptions
      - b) $40-70/month - Claude Code + Perplexity Pro, free tiers for rest [CORRECT]
      - c) $0 - only free tiers
      - d) $500+/month - enterprise plans
    correct: b
    explanation: |
      Recommended subscription stacks:  Minimal Stack ($40-70/month): - Claude Code (pay-per-use): $20-50 - Perplexity Pro: $20 - Everything else: Free tiers (NotebookLM, Kimi, Gemini free)  Balanced Stack ($80-110/month): - Add Gemini Advanced ($20) and Cursor Pro ($20)  Cost optimization tips: - Use Haiku for simple tasks - Batch research sessions in Perplexity - Check context usage regularly (/status) 

  - id: 15-011
    difficulty: senior
    category: "Unknown"
    question: |
      What cost savings does the Claude Code to LM Studio bridge provide?
    options:
      - a) 20-30% savings on API costs
      - b) 50% savings but slower execution
      - c) 80-90% savings by planning with Opus then executing free with local LLM [CORRECT]
      - d) No savings but better privacy
    correct: c
    explanation: |
      The Claude Code to LM Studio bridge provides 80-90% cost savings. Strategy: use Opus for planning phase (~$0.50-2 per session) then execute implementation with a free local LLM via LM Studio. The bridge script translates Claude's plan into local LLM instructions. Trade-off: local LLMs are less capable but free. 

  - id: 15-012
    difficulty: power
    category: "Unknown"
    question: |
      What are the 3 documented external orchestration systems for multi-agent Claude Code?
    options:
      - a) LangChain, AutoGPT, CrewAI
      - b) Vercel AI SDK, Fly.io Machines, Railway
      - c) Gas Town (Steve Yegge), multiclaude (dlorenc), agent-chat (Justin Abrahms) [CORRECT]
      - d) Kubernetes, Docker Compose, Terraform
    correct: c
    explanation: |
      Three documented external orchestration systems:  1. **Gas Town** (Steve Yegge) - Multi-agent coordination with shared context 2. **multiclaude** (dlorenc) - Parallel Claude Code instances with task distribution 3. **agent-chat** (Justin Abrahms) - Inter-agent communication protocol  These are community-built tools, not official Anthropic products. Each solves multi-agent coordination differently. 

  - id: 15-013
    difficulty: senior
    category: "Unknown"
    question: |
      What 3 areas should sub-agents audit when evaluating a skeleton project before forking?
    options:
      - a) Performance, Cost, Scalability
      - b) UI, API, Database
      - c) Security, Architecture, Developer Experience (DX) [CORRECT]
      - d) Tests, Documentation, CI/CD
    correct: c
    explanation: |
      Skeleton project evaluation uses 3 specialized sub-agents:  1. **Security** - Audit dependencies, secrets management, auth patterns, known vulnerabilities 2. **Architecture** - Evaluate code organization, patterns, scalability, maintainability 3. **Developer Experience (DX)** - Check setup complexity, documentation quality, tooling, onboarding friction  Each agent produces a score and recommendations before the fork decision. 

